{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Two: Classification #\n",
    "\n",
    "Billy Nayden\n",
    "Sean McWhirter\n",
    "Andrew Mejia\n",
    "Rajesh Salturi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.offline.init_notebook_mode()\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.svm import SVC\n",
    "from pandas.plotting import boxplot\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error, mean_squared_error\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "print(__doc__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CLASSIFICATION ANALYSIS HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://www.featureranking.com/tutorials/machine-learning-tutorials/information-gain-computation/\n",
    "#https://nbviewer.jupyter.org/github/jakemdrew/MachineLearningExtras/blob/master/LFW%20Dataset%20and%20Class%20Imbalance.ipynb\n",
    "\n",
    "def gini_index(y):\n",
    "    probs = pd.value_counts(y,normalize=True)\n",
    "    return 1 - np.sum(np.square(probs))\n",
    "\n",
    "def plot_class_dist(y, target_label = None):\n",
    "    fig, axarr = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    class_ct = len(np.unique(y))\n",
    "    vc = pd.value_counts(y)\n",
    "    print('Total Records', len(y))\n",
    "    print('Total Classes:', class_ct)\n",
    "    print('Class Gini Index', gini_index(y))\n",
    "    print('Smallest Class Id:',vc.idxmin(),'Records:',vc.min())\n",
    "    print('Largest Class Id:',vc.idxmax(),'Records:',vc.max())\n",
    "    print('Accuracy when Guessing:', np.round( (1 / len(np.unique(y))) * 100, 2), '%')\n",
    "\n",
    "    sns.distplot(y, ax=axarr[0], bins=class_ct).set_title('Target Class Distribution:', target_label);\n",
    "    sns.distplot(y, ax=axarr[1], kde=False, bins=class_ct).set_title('Target Class Counts:', target_label);\n",
    "    \n",
    "    \n",
    "# This function creates dummy encodings from a lsit of features of interest and returns a dataframe     \n",
    "def create_dummy_encod(ml_df,features_of_interest, drop_first_cat=True, sparsity=True): \n",
    "    tmp_cont = []\n",
    "    ml_data_copy = ml_df.copy()\n",
    "    for feat in features_of_interest: \n",
    "        tmp_df = pd.get_dummies(ml_data_copy[feat],prefix=str(feat),sparse=sparsity,drop_first=drop_first_cat)\n",
    "        tmp_cont.append(tmp_df)\n",
    "        feat_df = pd.concat(tmp_cont,axis=1)\n",
    "        ml_df = pd.concat([ml_data_copy,feat_df], axis=1)\n",
    "        ml_df = ml_df.drop(columns = features_of_interest, axis = 1)\n",
    "    return ml_df \n",
    "\n",
    "\n",
    "\n",
    "#Adopted from \n",
    "#https://nbviewer.jupyter.org/github/jakemdrew/MachineLearningExtras/blob/master/LFW%20Dataset%20and%20Class%20Imbalance.ipynb\n",
    "\n",
    "# This function provides a way to use stratiefied cv with the test models function below using the default score of accuarcy \n",
    "\n",
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "\n",
    "def stratified_cross_validate(model, X, y, cv=cv):\n",
    "    start = time.time()\n",
    "    cv_results = cross_validate(model, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "    elapsed_time = (time.time() - start) \n",
    "    print ('Fold Scores:')\n",
    "    print(' ')\n",
    "    print(cv_results['test_score'])\n",
    "    print(' ')\n",
    "    print('Mean Accuracy: ', cv_results['test_score'].mean())\n",
    "    print('Mean Fit Time: ', cv_results['fit_time'].mean())\n",
    "    print('Mean Score Time: ', cv_results['score_time'].mean())\n",
    "    print('CV Time: ', elapsed_time)\n",
    "    return\n",
    "\n",
    "\n",
    "#This function provides a quick method to to performe strativied cross validation model comparision \n",
    "\n",
    "def test_models(X, y, models, model_names):\n",
    "    for model, model_name in zip(models,model_names):\n",
    "        print(model_name)\n",
    "        print('--------------------------------')\n",
    "        stratified_cross_validate(model,X,y)\n",
    "        print(' ')\n",
    "\n",
    "#This function will plot PCs based on the length of features in the dataframe or change to how many features you wish to input \n",
    "def plot_pca(X,var_ratio_pcs = 60):\n",
    "    # Perform PCA on the data to reduce the number of initial features \n",
    "    # and to remove correlations that are common between pixel features \n",
    "    pca = PCA(n_components=X.shape[1])\n",
    "    pca.fit(X)\n",
    "\n",
    "    # Inspect the explained variances to determine how many components to use  \n",
    "    plt.subplots(figsize=(8, 8))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance');\n",
    "    print('Cumulative Explained variance explained with :', var_ratio_pcs , 'components:',sum(pca.explained_variance_ratio_[0:var_ratio_pcs]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Analysis Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adopted from https://github.com/jakemdrew/DataMiningNotebooks/blob/master/07.%20Regression.ipynb\n",
    "\n",
    "#Function for Root mean squared error\n",
    "#https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "def rmse(y_actual, y_predicted):\n",
    "    return np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "#Function for Mean Absolute Percentage Error (MAPE) - Untested\n",
    "#Adapted from - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "def mape(y_actual, y_predicted): \n",
    "    mask = y_actual != 0\n",
    "    return (np.fabs(y_actual - y_predicted)/y_actual)[mask].mean() * 100\n",
    "\n",
    "#Create scorers for rmse and mape functions\n",
    "mae_scorer = make_scorer(score_func=mean_absolute_error, greater_is_better=False)\n",
    "rmse_scorer = make_scorer(score_func=rmse, greater_is_better=False)\n",
    "mape_scorer = make_scorer(score_func=mape, greater_is_better=False)\n",
    "mse_scorer = make_scorer(score_func = mean_squared_error, greater_is_better=False)\n",
    "\n",
    "#Make scorer array to pass into cross_validate() function for producing mutiple scores for each cv fold.\n",
    "errorScoring = {'MAE':  mae_scorer, \n",
    "                'RMSE': rmse_scorer,\n",
    "                'MAPE': mape_scorer, \n",
    "                'MSE' : mse_scorer\n",
    "               } \n",
    "\n",
    "def EvaluateRegressionEstimator(regEstimator, X, y, cv):\n",
    "    \n",
    "    scores = cross_validate(regEstimator, X, y, scoring=errorScoring, cv=cv, return_train_score=True)\n",
    "\n",
    "    #cross val score sign-flips the outputs of MAE\n",
    "    # https://github.com/scikit-learn/scikit-learn/issues/2439\n",
    "    scores['test_MAE'] = scores['test_MAE'] * -1\n",
    "    scores['test_MAPE'] = scores['test_MAPE'] * -1\n",
    "    scores['test_RMSE'] = scores['test_RMSE'] * -1\n",
    "    scores['test_MSE'] = scores['test_MSE'] * -1\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    maeAvg = scores['test_MAE'].mean()\n",
    "    print_str = \"The average MAE for all cv folds is: \\t\\t\\t {maeAvg:.5}\"\n",
    "    print(print_str.format(maeAvg=maeAvg))\n",
    "\n",
    "    #print mean test_MAPE for all folds\n",
    "    scores['test_MAPE'] = scores['test_MAPE']\n",
    "    mape_avg = scores['test_MAPE'].mean()\n",
    "    print_str = \"The average MAE percentage (MAPE) for all cv folds is: \\t {mape_avg:.5}\"\n",
    "    print(print_str.format(mape_avg=mape_avg))\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    RMSEavg = scores['test_RMSE'].mean()\n",
    "    print_str = \"The average RMSE for all cv folds is: \\t\\t\\t {RMSEavg:.5}\"\n",
    "    print(print_str.format(RMSEavg=RMSEavg))\n",
    "    print('*********************************************************')\n",
    "    \n",
    "    #print mean MAE for all folds \n",
    "    MSEavg = scores['test_MSE'].mean()\n",
    "    print_str = \"The average MSE for all cv folds is: \\t\\t\\t {MSEavg:.5}\"\n",
    "    print(print_str.format(MSEavg=MSEavg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['MAE'] = scores['test_MAE']\n",
    "    scoresResults['MAPE'] = scores['test_MAPE']\n",
    "    scoresResults['RMSE'] = scores['test_RMSE']\n",
    "    scoresResults['MSE'] = scores['test_MSE']\n",
    "    return scoresResults\n",
    "\n",
    "class CappedLinearRegression(LinearRegression):\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.clip(super(CappedLinearRegression, self).predict(X), 1e5, 9.9e6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_white_space(cols_list, dataframe): \n",
    "    df = dataframe\n",
    "    for col in cols_list:\n",
    "        df[col] = df[col].str.strip()\n",
    "    return df \n",
    "    \n",
    "    \n",
    "def unique_categories(columns_list, dataframe_1): \n",
    "    miss_cat_vars = {}\n",
    "    for var in columns_list: \n",
    "        print(var)\n",
    "        k,v = var,dataframe_1[var].unique()\n",
    "        miss_cat_vars.update({k : v})\n",
    "    return miss_cat_vars\n",
    "\n",
    "#Adapted from Jezreal's answer \n",
    "#https://stackoverflow.com/questions/51189962/how-to-replace-0-values-with-mean-based-on-groupby\n",
    "\n",
    "def grouper_impute(dataframe_2, grouper_col = None, grouper_impute = None, replace_val = None, transfrmtn = None): \n",
    "\n",
    "    dataframe_2[grouper_impute] = dataframe_2[grouper_impute].replace(replace_val, np.nan)\n",
    "    dataframe_2[grouper_impute] = dataframe_2[grouper_impute].fillna(dataframe_2.groupby(grouper_col)[grouper_impute].transform(transfrmtn))\n",
    "    return dataframe_2\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Part 1\n",
    ">Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and prepare your class variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The data set we will be working with is the NYC property data sales from New York City for the 5 boroghs.\n",
    "\n",
    ">https://www.kaggle.com/new-york-city/nyc-property-sales\n",
    "\n",
    ">We wanted to create a classifier using this data set to predict which BOROUGH a property belonged to, and we wanted to create a regressor to predict a property's SALE PRICE.\n",
    "\n",
    "This dataset contains the location, address, type, sale price, and sale date of building units sold. A reference on the trickier fields:\n",
    "\n",
    "BOROUGH: A digit code for the borough the property is located in; in order these are Manhattan (1), Bronx (2), Brooklyn (3), Queens (4), and Staten Island (5).\n",
    "\n",
    "BLOCK; LOT: The combination of borough, block, and lot forms a unique key for property in New York City. Commonly called a BBL. We will not use this combination in our analysis, as we will treat Block and Lot as independent features, since it is a nominal label created using this type of key value pair.\n",
    "\n",
    "BUILDING CLASS AT PRESENT and BUILDING CLASS AT TIME OF SALE: The type of building at various points in time. See the glossary linked to below.\n",
    "\n",
    "Note that because this is a financial transaction dataset, there are some points that need to be kept in mind:\n",
    "\n",
    "There are sales with a $0 dollar value. These sales are actually transfers of deeds between parties: for example, parents transferring ownership to their home to a child after moving out for retirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_url = 'https://raw.githubusercontent.com/andrewmejia600/MSDS7331/master/RAW_DATA/nyc-rolling-sales.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(raw_data_url, encoding=\"utf-8\", converters = {'LAND SQUARE FEET': str.strip, 'GROSS SQUARE FEET' : str.strip, 'SALE PRICE': str.strip  } )\n",
    "raw_data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data attributes explainations are as follows: \n",
    "\n",
    "Unnamed: 0 - An index from the data source that is not defined \n",
    "\n",
    "BOROUGH - A digit code for the borough the property is located in; in order these are Manhattan (1), Bronx (2), Brooklyn (3), Queens (4), and Staten Island (5). \n",
    "\n",
    "NEIGHBOHOOD - The neighboorhood of the address \n",
    "\n",
    "BUILDING CLASS CATEGORY - Is the description of the building class, reference https://www1.nyc.gov/assets/finance/jump/hlpbldgcode.html \n",
    "\n",
    "TAX CLASS AT PRESENT - The Tax class of the building at the time when the data was collected \n",
    "\n",
    "BLOCK - The physical block of the address \n",
    "\n",
    "LOT - The lot on the the block \n",
    "\n",
    "EASE-MENT - A sparse column with no values in it, just white space. \n",
    "\n",
    "BUILDING CLASS AT PRESENT - The building class at the time of data collection \n",
    "\n",
    "ADDRESS - The Physical address of the building \n",
    "\n",
    "APARTMENT NUMBER - The Apartment Number of the unit \n",
    "\n",
    "ZIP CODE - Zip Code of the property\n",
    "\n",
    "RESIDENTIAL UNITS - The number of residential units to the building \n",
    "\n",
    "COMMERCIAL UNITS - The number of commerifal units to the building \n",
    "\n",
    "Total Units - The number of total units, the sum of Residential and Commercial Units to the building \n",
    "\n",
    "LAND SQUARE FEET - The physical foot print of square feet of the building \n",
    "\n",
    "GROSS SQUARE FEET -- The entire square footage of the buidling, the sum of the floor square footage for mutli-story buildings \n",
    "\n",
    "YEAR BUILT - The year of construction of the property \n",
    "\n",
    "TAX CLASS AT TIME OF SALE - The tax class of the building when the property sold \n",
    "\n",
    "Building CLASS AT TIME OF SALE - The building class at the time when the property sold \n",
    "\n",
    "SALE PRICE - The sale price of the building, $0 are indicative of family property transfers and '-' are where there is no known sale price \n",
    "\n",
    "SALE DATE - The sale date of the property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = raw_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are potentially 22 features for us to use for both the classification task of the data and the regression task of the data. We will drop the Unamed 0: column as it appears to be an index that is not of importance. \n",
    "\n",
    "EASE-MENT is another feature that is mostly sparse, and shows no real value as a feature. \n",
    "\n",
    "Address has many nominal values and will not serve as a sound predictor, and will drastically increase the number of features to be encoded, so we will drop this column as well.  \n",
    "\n",
    "Therefore the further analysis and model testing will be conducted without Unamed:0, EASE-MENT, and Address. We will also replace SALE DATE with DATEOFSALE as DATEOFSALE will be used for downline processing and will be in date time format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data['DATEOFSALE'] = pd.to_datetime(housing_data['SALE DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = housing_data.drop(columns = ['Unnamed: 0', 'EASE-MENT', 'APARTMENT NUMBER', 'SALE DATE'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are removing the whitespace from the column names to make them easier to manipulate in our modeling analysis and building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.columns = housing_data.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catagorical_vars = list(housing_data.select_dtypes(include='object').columns)\n",
    "print(catagorical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = remove_white_space(catagorical_vars, housing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are not sure what the '-' value for sale price means, we will omit it from further analysis, as these could be family property transfers or transfers other than sales where there is not a $0 value or there is no other data known for this observation. We see there are 14561 observations of this type. It must be stated, for future generalizations, the outcomes of these regression and classification models are limited to observations that do not fall into this category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(housing_data[housing_data['SALEPRICE'] == '-']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see this is also the case with LANDSQUAREFEET and GROSSSQUAREFEET as well and we see there are instances where there is a 0 zipcode. As there is no way to vet what the true zipcode of the property would be, without extensive searching, we will exclude these observations as well. We also see there are some properties with a 0 total units, so we will exclude these observations, as we do not want to artifically inflate the number of total units by adding to the column. With these exclusions, we must restate our previous disclaimer of the outcomes of these regression and classification models are limited to observations that do not fall into this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = housing_data[~((housing_data['LANDSQUAREFEET'] == '-') | (housing_data['GROSSSQUAREFEET'] == '-') | (housing_data['SALEPRICE'] == '-'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = housing_data[(housing_data['ZIPCODE'] != 0) & (housing_data['TOTALUNITS'] != 0) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = housing_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some attributes, such as LANDSQUREFEET, GROSSSUAREFEET and SALEPRICE are the wrong data type. \n",
    "We are changing the data types to SALEPRICE, LANDSQUAREFEET and GROSSSQUAREFEET to int64, and making ZIPCODE a nominal label, as ZIPCODE is more of a nominal label in this case rather than an ordinal value, since a higher ZIPCODE number does not necessiarily correspond to a more favorable property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html\n",
    "housing_data = housing_data.astype({'SALEPRICE': 'int64', 'LANDSQUAREFEET' : 'int64', 'GROSSSQUAREFEET' : 'int64', 'ZIPCODE' : 'object'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENGINEERED FEATURES AND IMPUTATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating an ordinal feature for when in the month the property sold, 1 being the first of the month, 2 being the middle of the month and 3 being the last of the month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_of_day = housing_data.DATEOFSALE.astype(str).str.slice(start=-2).astype(int)\n",
    "\n",
    "housing_data['TIMEOFMONTH'] = pd.cut(string_of_day, [1,14,15,31], labels=[1,2,3], include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.TIMEOFMONTH = housing_data.TIMEOFMONTH.astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reviewing https://www1.nyc.gov/assets/finance/jump/hlpbldgcode.html the New York Building class at present feature has many nominal features. We wanted a more general feature of what the building type is, i.e. whether the property is a multi family dwelling with vs  with out an elevator. \n",
    "\n",
    "\n",
    ">'DWELLING' -- DWELLING \n",
    "'MULTI_DWELLING_NO_ELV'-- Multifamily dwelling no elevator \n",
    "'MULTI_DWELLING_ELV' -- Multifamily dwelling with elevator\n",
    "'WAREHOUSE' -- Warehouse\n",
    "'FACTORY' -- Factory\n",
    "'GARAGE_P_LOT' -- Garage or Parking Lot\n",
    "'HOTEL' -- Hotel \n",
    "'HOSIPTAL' -- Hospital \n",
    "'THEATRE' -- Theatre\n",
    "'RETAIL' -- Retail \n",
    "'LOFT' -- Loft \n",
    "'RELIGIOUS' -- Religious \n",
    "'SOCIAL_INSTITUTION' -- Social Institution, i.e. an Asylum \n",
    "'OFFICE' -- Office \n",
    "'COMMUNITY_CENTER' -- Community Center\n",
    "'PUBLIC_REC' -- Public Recreation\n",
    "'COMMERCIAL' -- Commercial \n",
    "'DWELLING_RETAIL' -- Dwelling with retail \n",
    "'PORT' -- Port of entry \n",
    "'UTILITY' -- Utility, i.e. railroad \n",
    "'ZONED' -- Zoned, i.e. Courthouse\n",
    "'SCHOOL' -- Schools \n",
    "'PUBLIC_SAFETY' -- Public Safety, i.e. Firehouse \n",
    "'OTHER' -- Catchall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www1.nyc.gov/assets/finance/jump/hlpbldgcode.html#A\n",
    "BUILD_CLASS_DICT_ = {'A' : 'DWELLING', 'B' : 'DWELLING', 'C' : 'MULTI_DWELLING_NO_ELV', 'D' : 'MULTI_DWELLING_ELV', 'E' : 'WAREHOUSE', 'F' : 'FACTORY', 'G' : 'GARAGE_P_LOT', 'H' : 'HOTEL', 'I' : 'HOSIPTAL', 'J' : 'THEATRE', 'K' : 'REATAIL', 'L' : 'LOFT', 'M' : 'RELIGIOUS', 'N' : 'SOCIAL_INSTITUTION', 'O' : 'OFFICE', 'P' : 'COMMUNITY_CENTER', 'Q' : 'PUBLIC_REC', 'R' : 'COMMERCIAL' , 'S' : 'DWELLING_RETAIL', 'T' : 'PORT', 'U' : 'UTILITY', 'V' : 'ZONED', 'W' : 'SCHOOL', 'Y' : 'PUBLIC_SAFETY', 'Z' : 'OTHER'}\n",
    "\n",
    "housing_data['BUILDCLASSGENER'] = housing_data.BUILDINGCLASSATPRESENT.astype(str).str.slice(start=0, stop=1).map(BUILD_CLASS_DICT_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to create a copy of the sale price for use with our imputations for the regression of sale price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data['SALEPRICECPY'] = housing_data['SALEPRICE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall impute the $0 SALEPRICE feature, that are family transfers, with the median saleprice of the ZIPCODE. As we are assuming these values should be relatively close to each other if they are within the zipcode, since using Borogh could be too general. \n",
    "\n",
    "We will impute YEARBUILT, LANDSQUAREFEET and GROSSSQUREFEET with the median of the zipcode as well for similar reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = grouper_impute(dataframe_2 = housing_data, grouper_col = 'ZIPCODE', grouper_impute = 'SALEPRICE', replace_val = 0, transfrmtn = 'median')\n",
    "\n",
    "housing_data = grouper_impute(dataframe_2 = housing_data, grouper_col = 'ZIPCODE', grouper_impute = 'YEARBUILT', replace_val = 0, transfrmtn = 'median')\n",
    "\n",
    "\n",
    "housing_data = grouper_impute(dataframe_2 = housing_data, grouper_col = 'ZIPCODE', grouper_impute = 'LANDSQUAREFEET', replace_val = 0, transfrmtn = 'median')\n",
    "\n",
    "housing_data = grouper_impute(dataframe_2 = housing_data, grouper_col = 'ZIPCODE', grouper_impute = 'GROSSSQUAREFEET', replace_val = 0, transfrmtn = 'median')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will engineer a feature for the decade of when the property was built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.pad.html\n",
    "housing_data['DECADEBUILT'] = housing_data.YEARBUILT.astype(str).str.slice(start=0,stop=3).str.pad(width = 4, side='right', fillchar = str(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will engineer a feature for the age of the property at time of sale, by subtracting the date of sale year from the year built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data['BUILDAGE'] = housing_data.DATEOFSALE.astype(str).str.slice(start=0,stop=4).astype(int) - housing_data.YEARBUILT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cast SALEPRICECPY, LANDSQUREFEET, GROSSSQUAREFEET, YEARBUILT, RESIDENTIALUNITS, BUILDAGE as int64 and cast ZIPCODE as a nomial feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = housing_data.astype({'SALEPRICECPY': 'int64', 'LANDSQUAREFEET' : 'int64', \n",
    "                                    'GROSSSQUAREFEET' : 'int64', 'ZIPCODE' : 'object',\n",
    "                                   'YEARBUILT' : 'int64', 'RESIDENTIALUNITS' : 'int64'\n",
    "                                   ,'BUILDAGE' : 'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accounting for duplicate records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are 577 duplicate records, as we are not sure if these are errors, or are really different properties, we will omit these observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_records = housing_data[housing_data.duplicated(keep = False)]\n",
    "duplicate_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Duplicate Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = housing_data[~housing_data.duplicated(keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time we will begin to formalize the approach for our experiment. \n",
    "\n",
    "We will create one dataset for the classification task, and one data set for the regression task. \n",
    "\n",
    "The reason for this, is as we performed some preliminary exploratory data analysis and model building, we saw the SALEPRICE attribute has a dynamic range of 0 to 2.2e9. As a result, prelminary regression models had a mean absolute error of ~1e6. There were many observations that were high leverage outliers that were driving the mean absolute error to ~1e6. It was decided this was not a practical method of predicting property values.  \n",
    "\n",
    "Susbequent research should be done to consider contstructing models for regression of property values based on futher property value segmetation, i.e. 0 to 1e5, 1.01e5 to 1e6, 1.01e6 to 1.01e8. For this experiment, we will generalize our dynamic range of property values to be within 1e5 to 1e7, therefore any model findings are limited to property values within the range of 1e5 to 1e7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feat = list(housing_data.select_dtypes(include='object').columns)\n",
    "cat_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Part 2\n",
    ">Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The final data set for the classification task will include the following features: \n",
    "\n",
    ">NEIGHBORHOOD, BUILDINGCLASSCATEGORY , ZIPCODE, DECADEBUILT, TAXCLASSATPRESENT, \n",
    "BUILDINGCLASSATPRESENT, TIMEOFMONTH, BUILDCLASSGENER, RESIDENTIALUNITS, COMMERCIALUNITS, \n",
    "TOTALUNITS, LANDSQUAREFEET, GROSSSQUAREFEET, YEARBUILT, SALEPRICE \n",
    "\n",
    ">With BOROUGH as the target. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from our previous discussion, our Sale Price, in this case the SALEPRICECPY attribute will only have a dynamic range of 1e5 to 9.9e6, as to make our prediction error metric of mean absolute error more reasonable. \n",
    "\n",
    ">The final data set for the regression task will include the following features:\n",
    "\n",
    "\n",
    ">NEIGHBORHOOD,BUILDINGCLASSCATEGORY, ZIPCODE, DECADEBUILT, TAXCLASSATPRESENT, \n",
    "BUILDINGCLASSATPRESENT, TIMEOFMONTH, BUILDCLASSGENER, RESIDENTIALUNITS, COMMERCIALUNITS, \n",
    "TOTALUNITS, LANDSQUAREFEET, GROSSSQUAREFEET, YEARBUILT, BUILDAGE, BOROUGH\n",
    "\n",
    ">With SALEPRICECPY as the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After conducting our imputations based by ZIPCODE and data exclusions, \n",
    "The final data attributes for both the regression and classification tasks explainations are as follows:\n",
    "\n",
    "BOROUGH - A digit code for the borough the property is located in; in order these are Manhattan (1), Bronx (2), Brooklyn (3), Queens (4), and Staten Island (5).\n",
    "\n",
    "NEIGHBOHOOD - The neighboorhood of the address\n",
    "\n",
    "BUILDING CLASS CATEGORY - Is the description of the building class, reference https://www1.nyc.gov/assets/finance/jump/hlpbldgcode.html\n",
    "\n",
    "TAXCLASSATPRESENT - The Tax class of the building at the time when the data was collected\n",
    "\n",
    "BUILDINGCLASSATPRESENT - The building class at the time of data collection\n",
    "\n",
    "ZIPCODE - Zip Code of the property\n",
    "\n",
    "DECADEBUILT - Engineered feature for the decade of when the property was built \n",
    "\n",
    "TIMEOFMONTH - Engineered feature for when in the month the property was sold\n",
    "\n",
    "BUILDCLASSGENER - Engineered feature of the building class categegory based on https://www1.nyc.gov/assets/finance/jump/hlpbldgcode.html\n",
    "\n",
    "RESIDENTIAL UNITS - The number of residential units to the building\n",
    "\n",
    "COMMERCIAL UNITS - The number of commerifal units to the building\n",
    "\n",
    "Total Units - The number of total units, the sum of Residential and Commercial Units to the building\n",
    "\n",
    "LAND SQUARE FEET - The physical foot print of square feet of the building\n",
    "\n",
    "GROSS SQUARE FEET - The entire square footage of the buidling, the sum of the floor square footage for mutli-story buildings\n",
    "\n",
    "YEARBUILT - The year of construction of the property\n",
    "\n",
    "BUILDAGE - Engineered feature for regression only - the age of the building\n",
    "\n",
    "SALEPRICE - Classification Only - The sale price of the building, 0 are indicative of family property transfers and '-' are where there is no known sale price and the $0 values were imputed with the zipecode median \n",
    "\n",
    "SALEPRICECPY - Regression Only - The engineered feature of SALEPRICE, but we impute the 0 value using the zipcode median only after we have we capped the property values from the data to be 1e5 to 9.9e6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION TASK DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_ml_df_classification = housing_data[['NEIGHBORHOOD','BUILDINGCLASSCATEGORY','ZIPCODE', 'DECADEBUILT', 'TAXCLASSATPRESENT', \n",
    "                              'BUILDINGCLASSATPRESENT', 'TIMEOFMONTH','BUILDCLASSGENER', \n",
    "                              'RESIDENTIALUNITS', 'COMMERCIALUNITS', 'TOTALUNITS',\n",
    "                              'LANDSQUAREFEET', 'GROSSSQUAREFEET','YEARBUILT',\n",
    "                              'SALEPRICE','BOROUGH']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_ml_df_classification.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression TASK DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_regression = housing_data[(housing_data.SALEPRICECPY > 1e5) & (housing_data.SALEPRICECPY < 9.9e6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_regression = housing_data_regression.drop(columns = ['SALEPRICE'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_regression = grouper_impute(dataframe_2 = housing_data_regression, grouper_col = 'ZIPCODE', grouper_impute = 'SALEPRICECPY', replace_val = 0, transfrmtn = 'median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_ml_df_regression = housing_data_regression[['NEIGHBORHOOD','BUILDINGCLASSCATEGORY','ZIPCODE', 'DECADEBUILT', 'TAXCLASSATPRESENT', \n",
    "                              'BUILDINGCLASSATPRESENT', 'TIMEOFMONTH','BUILDCLASSGENER', \n",
    "                              'RESIDENTIALUNITS', 'COMMERCIALUNITS', 'TOTALUNITS',\n",
    "                              'LANDSQUAREFEET', 'GROSSSQUAREFEET','YEARBUILT',\n",
    "                              'SALEPRICECPY','BUILDAGE','BOROUGH']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_ml_df_regression.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corr plot\n",
    "corr=housing_data.corr()\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(corr[(corr>= 0.1) | (corr <= -0.4)], cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1, annot=True, annot_kws={\"size\":11},ax=ax, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all of our int64 variables and examine histograms\n",
    "num_features = housing_data.loc[:, housing_data.dtypes == np.int64]\n",
    "num_features.hist(figsize=(16, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIS TAKES FOREVER TO RUN\n",
    "\n",
    "Frequency plots for the cateorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counts of the different categorical attributes\n",
    "#Code derived from a Toward Data Science blog (link:https://towardsdatascience.com/how-to-perform-exploratory-data-analysis-with-seaborn-97e3413e841d)\n",
    "\n",
    "\n",
    "#fig, ax=plt.subplots(3, 3, figsize=(20, 30))\n",
    "#for attribute, subplot in zip(housing_data, ax.flatten()):\n",
    "    #sns.countplot(housing_data[attribute], ax=subplot)\n",
    "    #for label in subplot.get_xticklabels():\n",
    "        #label.set_rotation(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many classes we have and their frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_num_class = housing_data.BOROUGH.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see we have a highly imbalanced data set for Borough values for the outcomes and there is a 20% chance of guessing the class correctly. \n",
    "> Couldn't we get 47% just by ugessing 3 ever time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage of each borough -- if we just guessed 4 every time, what would our accuracy be?\n",
    "housing_data['BOROUGH'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is zipcode going to do the entire job for us, or is there some crossover?\n",
    "sns.stripplot(x=housing_data['ZIPCODE'], y=housing_data['BOROUGH']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_dist(y_num_class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_of_int = [ 'NEIGHBORHOOD', 'BUILDINGCLASSCATEGORY', 'ZIPCODE', 'DECADEBUILT','TAXCLASSATPRESENT', 'BUILDINGCLASSATPRESENT', 'TIMEOFMONTH','BUILDCLASSGENER']\n",
    "housing_ml_df = housing_data[['NEIGHBORHOOD','BUILDINGCLASSCATEGORY','ZIPCODE', 'DECADEBUILT', 'TAXCLASSATPRESENT', \n",
    "                              'BUILDINGCLASSATPRESENT', 'TIMEOFMONTH','BUILDCLASSGENER', \n",
    "                              'RESIDENTIALUNITS', 'COMMERCIALUNITS', 'TOTALUNITS',\n",
    "                              'LANDSQUAREFEET', 'GROSSSQUAREFEET','YEARBUILT',\n",
    "                              'SALEPRICE','BOROUGH']]\n",
    "ml_df_enc = create_dummy_encod(ml_df = housing_ml_df,features_of_interest = feat_of_int, drop_first_cat=True, sparsity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df_enc.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 1\n",
    ">Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we will be creating models for two different tasks, classification and regression, we will need to use two separate metrics to evaluate our models.\n",
    "\n",
    "###### Classification\n",
    "For our classification model, our sole concern is accuracy.  We need our model to be able to predict the correct borough with high accuracy in order for our model to be deployable.  Given that this is a multi-class classificaiton issue, our \"guessing accuracy\" is relatively low, so as long as our accuracy is high and our confusion matrix doesn't show any anomolies, we can be confident in the accuracy metric alone.  \n",
    "\n",
    "###### Regression\n",
    "In order to evaluate our regression model, we will want to use a meric that not only provides meaningful insight into the model's performance, but is also interpretable.  With these two parameters in mind, the metric we will use for our regression models is Mean Absolute Error (MAE).  It will allow us to conduct a meaningful evaluation, and being able to use the MAE value in terms of dollars (SALEPRICE being our regression target), making it interpretable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 2\n",
    ">Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Classification\n",
    "This model uses 10-fold cross validation because our 70/30 train test split is subject to increased variance if we were to use a single holdout set. 10-fold cross validation helps reduce that variance.\n",
    "###### Regression\n",
    "To predict SALEPRICECPY, 10-fold cross valdiation will be used with a 80/20 train test split to account for the variance in the mean of the MAE of SALEPRICECPY. We will do this since the dynamic range of the property values are quite large, and we want to eliminate as much variance due to chance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 3\n",
    ">*Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Section Sean/Billy Edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create scaler object \n",
    "ML_std_scalr = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our dataset gives us when we don't scale the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break our data into 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=959, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    KNN Iteration 1 - No scaling/Stratified CV (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how KNN does without scaling our data first. We will use this as our baseline accuracy for this classification problem. The selection of 3 neighbors is arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://nbviewer.jupyter.org/github/jakemdrew/DataMiningNotebooks/blob/master/06.%20Classification.ipynb\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "model = KNeighborsClassifier(n_neighbors=3, weights='uniform', metric='euclidean')\n",
    "stratified_cross_validate(model, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like about 60% without scaling our data and using an arbitrary K value.\n",
    "\n",
    "Now let's loop through and find our optimal K to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "for K in range(1, 6):\n",
    "    model = KNeighborsClassifier(n_neighbors=K, weights='uniform', metric='euclidean')\n",
    "    print(\"K Value:\", K)\n",
    "    stratified_cross_validate(model, X, y, cv=cv)\n",
    "    print(\"\\n\")\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on `Mean Accuracy` 5 is the best K value for our unscaled model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will scale the data and re-run our loop to examine accuracy and identify the optimal K value.\n",
    "\n",
    "###  KNN Iteration 2 - scaling/Stratified CV (10)\n",
    "\n",
    "#### Warning: Takes about 5 mins to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data is necessary in this instance because KNN is a distance based model and will give higher weight to the variables with a larger scale. The below function brings the data into the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data\n",
    "X = ML_std_scalr.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find the optimal K value for our model with scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "for K in range(1, 6):\n",
    "    model = KNeighborsClassifier(n_neighbors=K, weights='uniform', metric='euclidean')\n",
    "    print(\"K Value:\", K)\n",
    "    stratified_cross_validate(model, X, y, cv=cv)\n",
    "    print(\"\\n\")\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on `Mean Accuracy`, 1 is the best K value for our scaled model with an accuracy of 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot PCA \n",
    "plot_pca(X,var_ratio_pcs = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it doesn't drastically reduce our dimensionality. It takes 500 components to explain 98.92% of the variance.  Unless we run into any major roadblocks with accuracy, we can just continue with scaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS TAKES A VERY LONG TIME--I STOPPED THE KERNEL AT 15 MINUTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PCA pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "\n",
    "# create cross validation iterator\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# setup pipeline to take PCA, then fit a KNN classifier\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA_Eric',PCA(n_components=500,svd_solver='randomized')),\n",
    "     ('CLF_Eric',KNeighborsClassifier(n_neighbors=3))]\n",
    ")\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,y):\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print ('KNN, pipeline accuracy', total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Iteration 1 - CV (10)\n",
    "\n",
    "Since our dataset is sparse due to the OneHotEncoding, we will see whether Multinomial or Bernoulli is best.  The GausianNB will likely not be as effective due to the large number of zeros in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code resets the data set to the original, unscaled values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's establish a baseline accuracy for the unscaled data, using the Multinomial Naive Bayes model. We will test across alpha values of 0.001, 0.5, and 1 to find the best alpha value. Additionally, we will use 10-fold cross validation to reduce the variance in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's check multinomial\n",
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "for a in [0.001, 0.5, 1]:\n",
    "    model= MultinomialNB(alpha=a)\n",
    "    print(\"alpha value:\", a)\n",
    "    stratified_cross_validate(model, X, y, cv=cv)\n",
    "    print(\"\\n\")\n",
    "    print(\"------------------------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that an alpha value of 0.001 is best for our Multinomial Naive Bayes model and produces an accuracy rating of 48.6%, far lower than our KNN model with unscaled data. \n",
    "\n",
    "Next, we will rerun the 10-fold cross validation with scaled data.  Due to the fact that StandardScalar can produce negative values, which does not work with Niave Bayese, we will use MinMaxScalar to function with the Niave Bayes models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Iteration 2 - Scaled Data/Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Multinomial with scaled data\n",
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "#scale data using MinMaxScaler (Regular scaler provided negative numbers)\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "#multinomialNB\n",
    "for a in [0.001, 0.5, 1]:\n",
    "    model= MultinomialNB(alpha=a)\n",
    "    print(\"alpha value:\", a)\n",
    "    stratified_cross_validate(model, X, y, cv=cv)\n",
    "    print(\"\\n\")\n",
    "    print(\"------------------------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After iterating through various alpha values, it appears that MultinomialNB performs better with a lower alpha value (less smoothed data).  An alpha value of 0.001 produced a mean 10-fold cross-validated accuracy of 99.97%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now test Bernoulli using a static alpha of 0.001, but testing binarize values of 0.001, 0.5, and 1. Again, we will use 10-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Iteration 3 - Scaled Data/Bernoulli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "#BernoulliNB\n",
    "#We have already seen that we do not want a large alpha value for smoothing\n",
    "#Let's iterate through the BernoulliNB with some different binarize values to see what produces the best results.\n",
    "for b in [0.001, 0.5, 1]:\n",
    "    model= BernoulliNB(alpha=0.001, binarize=b)\n",
    "    print(\"binarize value:\", b)\n",
    "    stratified_cross_validate(model, X, y, cv=cv)\n",
    "    print(\"\\n\")\n",
    "    print(\"------------------------\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarize values of 0.001 and 0.5 both produce very high accuracy values, but the binarize value of 0.001 performs slightly better in terms of time it takes to run, likely because it does not have to binarize more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the confusion matrix to ensure that our high accuracy numbers are not disproportionately affected by a particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With BernoulliNB and a binarize value of 0.001, we get an accuracy of 99.95%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Iteration 1 - Unscaled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we reset our data to the original, unscaled data set to use for our initial logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset Data after PCA\n",
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will once again use 10-fold cross validation to determine a baseline accuracy score for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://nbviewer.jupyter.org/github/jakemdrew/MachineLearningExtras/blob/master/LFW%20Dataset%20and%20Class%20Imbalance.ipynb\n",
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "model = LogisticRegression(solver='lbfgs', random_state=959, max_iter = 1e5)\n",
    "stratified_cross_validate(model, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 48.7% as our baseline accuracy for logistic regression. Let's take a look at the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Iteration 2 - Scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we will scale the features, in hopes of improving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset X and y\n",
    "# Break our data into 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=959, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the features will likely make a big improvement to accuaracy \n",
    "X = ML_std_scalr.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run the model using 10-fold cross validation on the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "model = LogisticRegression(solver='lbfgs', random_state=959, max_iter = 1e5)\n",
    "stratified_cross_validate(model, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data drastically improves the accuracy of our model. With scaled data the `Mean Accuracy` of our logistic regression model is 88.8%. While it takes a bit longer to fit than the unscaled model, the improvement in accuracy is worth the CV Time tradeoff. Let's run the basic parameters through a SearchGrid and see if that helps the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Iteration 3 - GridSearch on Scaled Data\n",
    "\n",
    "##### GridSearch to Identify the best parameters\n",
    "\n",
    "Due to the number of combinations, we will use 3-fold cross validation so the CV time is not prohibitiely long.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Logistic Regression object and perform a grid search to find the best parameters\n",
    "#Adopted from https://github.com/jakemdrew/DataMiningNotebooks/blob/master/07.%20Regression.ipynb\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "penalty = ['l2']\n",
    "C = (1, 5, 10)\n",
    "solver = ['sag', 'saga', 'lbfgs']\n",
    "\n",
    "parameters = dict(C=C, penalty=penalty, solver=solver, max_iter = 1e5)\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "logregGridSearch = GridSearchCV(estimator=logreg\n",
    "                   , verbose=0 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=3 # KFolds = 10 \n",
    "                   , n_jobs = -1\n",
    "                               )\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "logregGridSearch.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adopted from https://www.kaggle.com/enespolat/grid-search-with-logistic-regression\n",
    "#Print out our best parameters to plug into our logistic regression model\n",
    "\n",
    "print(\"The optimal parameters according to the gridsearch are: \" ,logregGridSearch.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll re-run our logistic regression model after discovering the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "logreg_clf = LogisticRegression(penalty='l2', C=10, solver='lbfgs', random_state=959, max_iter = 1e5)\n",
    "stratified_cross_validate(logreg_clf, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that tuning the parameters based on the GridSearch increased our accuracy even closer to 100%. Similarly, it has decreaed the CV time form over 100 seconds to 42 seconds. We will use the tuned Logistic Regression iteration as our final classifier in the comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_regr = housing_data[(housing_data.SALEPRICECPY > 1e5) & (housing_data.SALEPRICECPY < 9.9e6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_regr = housing_data_regr.drop(columns = ['SALEPRICE'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_regr = grouper_impute(dataframe_2 = housing_data_regr, grouper_col = 'ZIPCODE', grouper_impute = 'SALEPRICECPY', replace_val = 0, transfrmtn = 'median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_of_int = [ 'NEIGHBORHOOD', 'BUILDINGCLASSCATEGORY', 'ZIPCODE', 'DECADEBUILT','TAXCLASSATPRESENT', 'BUILDINGCLASSATPRESENT', 'TIMEOFMONTH','BUILDCLASSGENER']\n",
    "housing_ml_df_regr = housing_data_regr[['NEIGHBORHOOD','BUILDINGCLASSCATEGORY','ZIPCODE', 'DECADEBUILT', 'TAXCLASSATPRESENT', \n",
    "                              'BUILDINGCLASSATPRESENT', 'TIMEOFMONTH','BUILDCLASSGENER', \n",
    "                              'RESIDENTIALUNITS', 'COMMERCIALUNITS', 'TOTALUNITS',\n",
    "                              'LANDSQUAREFEET', 'GROSSSQUAREFEET','YEARBUILT',\n",
    "                              'SALEPRICECPY','BUILDAGE','BOROUGH']]\n",
    "ml_df_enc_regr = create_dummy_encod(ml_df = housing_ml_df_regr,features_of_interest = feat_of_int, drop_first_cat=True, sparsity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_regr = ml_df_enc_regr['SALEPRICECPY'].values\n",
    "X_regr = ml_df_enc_regr.drop(columns = ['SALEPRICECPY'], axis = 1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capped Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create scaler object \n",
    "ML_std_scalr = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Linear Regression object and perform a grid search to find the best parameters\n",
    "#Adopted from https://github.com/jakemdrew/DataMiningNotebooks/blob/master/07.%20Regression.ipynb\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=959)\n",
    "\n",
    "ML_std_scalr.fit(X_regr)\n",
    "\n",
    "X_regr_scl = ML_std_scalr.transform(X_regr)\n",
    "\n",
    "\n",
    "linreg = CappedLinearRegression()\n",
    "parameters = {'normalize':(True,False), 'fit_intercept':(True,False)}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=linreg\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X_regr_scl,y_regr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adopted from https://github.com/jakemdrew/DataMiningNotebooks/blob/master/07.%20Regression.ipynb\n",
    "\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create CappedLinearRegression predictions between 0 and 100% using the best parameters for our Linear Regression object\n",
    "#Adopted from https://github.com/jakemdrew/DataMiningNotebooks/blob/master/07.%20Regression.ipynb\n",
    "\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=959)\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "scoresResults_CappedReg =  EvaluateRegressionEstimator(regEstimator, X_regr_scl,y_regr, cv)\n",
    "\n",
    "mae_avg_CappedLinearRegression = scoresResults_CappedReg['MAE'].mean()\n",
    "scoresResults_CappedReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adopted from https://github.com/jakemdrew/DataMiningNotebooks/blob/master/07.%20Regression.ipynb\n",
    "\n",
    "ML_std_scalr.fit(X_regr)\n",
    "\n",
    "X_regr_scl = ML_std_scalr.transform(X_regr)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=959)\n",
    "reg = Lasso(fit_intercept=True, normalize=True,copy_X=True\n",
    "          , max_iter=10000, precompute=True, tol=0.0001, random_state=0)\n",
    "\n",
    "#Test parameters \n",
    "alpha = [0.001, 0.1, 1, 10, 20]\n",
    "selection = ['cyclic','random']\n",
    "warm_start = [True, False]\n",
    "parameters = {'alpha': alpha, 'selection': selection, 'warm_start': warm_start}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=reg\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X_regr_scl,y_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adopted from https://github.com/jakemdrew/DataMiningNotebooks/blob/master/07.%20Regression.ipynb\n",
    "\n",
    "#Create a regression estimator with best parameters for cross validation\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=959)\n",
    "\n",
    "regEstimator = Lasso(alpha=10, copy_X=True, fit_intercept=True, max_iter=10000, normalize=True,\n",
    "      positive=False, precompute=True, random_state=0, selection='random',\n",
    "      tol=0.0001, warm_start=True)\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "scoresResults_Lasso = EvaluateRegressionEstimator(regEstimator, X_regr,y_regr, cv)\n",
    "\n",
    "mae_avg_Lasso = scoresResults_Lasso['MAE'].mean()\n",
    "scoresResults_Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Adopted from https://github.com/jakemdrew/DataMiningNotebooks/blob/master/07.%20Regression.ipynb\n",
    "\n",
    "ML_std_scalr.fit(X_regr)\n",
    "\n",
    "\n",
    "X_regr_scl = ML_std_scalr.transform(X_regr)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=959)\n",
    "linreg = GradientBoostingRegressor()\n",
    "\n",
    "parameters = { 'loss' : ['ls']\n",
    "              ,'learning_rate' : [1e-3, 1e-1]\n",
    "              ,'n_estimators': [500] \n",
    "              ,'criterion': ['mae']\n",
    "              ,'min_samples_split':[2,3,4,5]\n",
    "              ,'min_samples_leaf': [10, 25, 50]\n",
    "              ,'max_features' : ['auto']\n",
    "              ,'subsample' : [1e-2]\n",
    "              ,'random_state': [0]\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "              \n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=linreg\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X_regr_scl , y_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adopted from https://github.com/jakemdrew/DataMiningNotebooks/blob/master/07.%20Regression.ipynb\n",
    "#Display the best estimator parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adopted from https://github.com/jakemdrew/DataMiningNotebooks/blob/master/07.%20Regression.ipynb\n",
    "\n",
    "#Create a regression estimator with best parameters for cross validation\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "scoresResults_GradientBoostingRegressor = EvaluateRegressionEstimator(regEstimator, X_regr_scl , y_regr, cv)\n",
    "\n",
    "\n",
    "mae_avg_GradientBoostingRegressor = scoresResults_GradientBoostingRegressor['MAE'].mean()\n",
    "scoresResults_GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 4\n",
    ">Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "##### Confusion matrix for KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix for K=1\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "\n",
    "# Break our data into 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=959, stratify=y)\n",
    "\n",
    "yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=1, weights='uniform', metric='euclidean')\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,y):\n",
    "    knn_clf.fit(X[train],y[train])\n",
    "    yhat[test] = knn_clf.predict(X[test])\n",
    "\n",
    "\n",
    "#confusion matrix for our KNN iteration 1\n",
    "cm = mt.confusion_matrix(y, yhat)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.imshow(cm_normalized,cmap=plt.get_cmap('coolwarm'),aspect='auto')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the confusion matrix above, the model primarily misclassifies the target bouroughs as bourough 3 (2 on the confusion matrix).  This makes sense as observations for borough 3 were more than 40% of our dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confustion Matrix for MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset Data for MinMaxScalar\n",
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values\n",
    "\n",
    "#Scale Data\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Break our data into 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=959, stratify=y)\n",
    "\n",
    "#confusion matrix for our Logistic Regression iteration 2\n",
    "mnb_clf= MultinomialNB(alpha=0.001)\n",
    "mnb_clf.fit(X_train, y_train)\n",
    "y_pred = mnb_clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Create a confusion matrix to see what classes the logistic regression is getting wrong \n",
    "\n",
    "\n",
    "plt.subplots(figsize=(10, 10))\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.imshow(cm_normalized,cmap=plt.get_cmap('coolwarm'),aspect='auto')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the confusion matrix for the MultinomialNB model, we can see that the misclassification issue with borough 3 is no longer present.  This makes sense as the imporovement in accuracy above the KNN model would have been improvements in the correct classificaiton of those boroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix for Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset Data for StandardScalar\n",
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values\n",
    "\n",
    "#scale data\n",
    "X = ML_std_scalr.fit_transform(X)\n",
    "\n",
    "# Break our data into 70% training and 30% test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=959, stratify=y)\n",
    "\n",
    "#confusion matrix for our Logistic Regression iteration 2\n",
    "lr_clf = LogisticRegression(penalty='l2', C=10, solver='lbfgs', random_state=959, max_iter = 1e5)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "\n",
    "# Create a confusion matrix to see what classes the logistic regression is getting wrong \n",
    "\n",
    "plt.subplots(figsize=(10, 10))\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.imshow(cm_normalized,cmap=plt.get_cmap('coolwarm'),aspect='auto')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can observe with the confusion matrix above is similar to the confusion matrix we saw with the MultinomialNB model.  This color scale can easily detect misclassifications (as well as show our school spirit), and it would follow that the confusion matrix appears pritine as the tuned logistic regression model provided a 10-fold cross-validated score of nearly 100%.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 5\n",
    ">*Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniquesbe sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "#### Statistical Analysis of KNN vs Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-15 statcompare.py\n",
    "clf1 = knn_clf #Top performing KNN Model\n",
    "\n",
    "#Reset Data for MinMaxScalar\n",
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values\n",
    "\n",
    "#Scale Data\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "                         \n",
    "clf2 = mnb_clf #Top performing Naive Bayes\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# is clf1 better or worse than clf2?\n",
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "acc1 = cross_val_score(clf1, X, y=y, cv=cv)\n",
    "acc2 = cross_val_score(clf2, X, y=y, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction for multiple tests\n",
    "\n",
    "Since we are doing two concurent hypothesis tests, we will do a Bonferroni correction by dividing alpha by 2 (0.5/2=0.025).  Our new critical t-value becomes 2.76\n",
    ">T-value derived from: https://www.statisticshowto.com/tables/t-distribution-table/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 19-28 statcompare.py\n",
    "t = 2.76 / np.sqrt(10)\n",
    "\n",
    "e = (1-acc1)-(1-acc2)\n",
    "# std1 = np.std(acc1)\n",
    "# std2 = np.std(acc2)\n",
    "stdtot = np.std(e)\n",
    "\n",
    "dbar = np.mean(e)\n",
    "print ('Range of:', dbar-t*stdtot,dbar+t*stdtot )\n",
    "print (np.mean(acc1), np.mean(acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h0: The mean accuracy of KNN and the mean acuracy of Multinomial Naive Bayes is the same\n",
    "ha: The mean accuracies of the two models are different\n",
    "\n",
    "\n",
    "###### Conclusion 1:\n",
    "Given the confidence interval of the differnece does not include zero, we have sufficient evidence to reject h0.  We are 95% confident that the mean accuracy of MultinomialNB is between 10.39% and 17.95% higher than KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there is no statistical difference between the accuracies of Multinomial Naive Bayes and Logistic Regression, we can look at secondary metrics, such as runtime.  In our model classification iterations, Multinomial Naive Bayes had a much lower cross validation time. Niave Bayes took only 3-5 seconds verus over 100 seconds with Logistic Regression.  Although the speed of our model isn't necessarily instrumental for deployment, it makes sense to utilize a faster model with the same accuracy--Niave Bayes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Comparison ov Naive Bayes vs Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset Data for MinMaxScalar\n",
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values\n",
    "\n",
    "#Scale Data\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "clf1 = mnb_clf #Top performing Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-15 statcompare.py\n",
    "\n",
    "\n",
    "#clf1 = mnb_clf #Top performing Naive Bayes\n",
    "\n",
    "#Reset Data for StandardScalar to use Logistic Regression\n",
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values\n",
    "\n",
    "#scale data\n",
    "X = ML_std_scalr.fit_transform(X)\n",
    "\n",
    "clf1 = lr_clf #Top performing logistic Regression model\n",
    "\n",
    "#Reset Data for MinMaxScalar\n",
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values\n",
    "\n",
    "#Scale Data\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "                         \n",
    "clf2 = mnb_clf #Top performing Naive Bayes\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# is clf1 better or worse than clf2?\n",
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "acc1 = cross_val_score(clf1, X, y=y, cv=cv)\n",
    "acc2 = cross_val_score(clf2, X, y=y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 19-28 statcompare.py\n",
    "t = 2.76 / np.sqrt(10)\n",
    "\n",
    "e = (1-acc1)-(1-acc2)\n",
    "# std1 = np.std(acc1)\n",
    "# std2 = np.std(acc2)\n",
    "stdtot = np.std(e)\n",
    "\n",
    "dbar = np.mean(e)\n",
    "print ('Range of:', dbar-t*stdtot,dbar+t*stdtot )\n",
    "print (np.mean(acc1), np.mean(acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h0: The mean accuracy of Multinomial Naive Bayes and the mean acuracy of Logistic Regression is the same\n",
    "\n",
    "ha: The mean accuracies of the two models are different\n",
    "\n",
    "Given the confidence interval of the differnece includes zero, we do not have sufficient evidence to reject h0.  Therefore, we can not confidently conclude that Mulinomial Naive Bayes and Logistic Regression have different mean accuracies.\n",
    "\n",
    "Given that there is no statistical difference between the accuracies of Multinomial Naive Bayes and Logistic Regression, we can look at secondary metrics, such as runtime.  In our model classification iterations, Multinomial Naive Bayes had a much lower cross validation time. Niave Bayes took only 3-5 seconds verus over 100 seconds with Logistic Regression.  Although the speed of our model isn't necessarily instrumental for deployment, it makes sense to utilize a faster model with the same accuracy--Niave Bayes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from \n",
    "#https://github.com/jakemdrew/DataMiningNotebooks/blob/master/06.%20Classification.ipynb\n",
    "\n",
    "#Gradient Bosting\n",
    "regr1 = GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='mae', init=None,\n",
    "                          learning_rate=0.1, loss='ls', max_depth=3,\n",
    "                          max_features='auto', max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=10, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "                          n_iter_no_change=None, presort='deprecated',\n",
    "                          random_state=0, subsample=0.01, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "\n",
    "\n",
    "\n",
    "regest1 = Pipeline(\n",
    "    [('scaler',ML_std_scalr),\n",
    "     ('regr',regr1)]\n",
    ")\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=959)\n",
    "reg_err1 = cross_val_score(regest1, X_regr, y_regr, cv=cv, scoring = 'neg_mean_absolute_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso\n",
    "regr2 = Lasso(alpha=10, copy_X=True, fit_intercept=True, max_iter=10000, normalize=True,\n",
    "      positive=False, precompute=True, random_state=0, selection='random',\n",
    "      tol=0.0001, warm_start=True)\n",
    "\n",
    "regest2 = Pipeline(\n",
    "    [('scaler',ML_std_scalr),\n",
    "     ('regr',regr2)]\n",
    ")\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=959)\n",
    "\n",
    "reg_err2 = cross_val_score(regest2, X_regr, y_regr, cv=cv, scoring = 'neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from \n",
    "#https://github.com/jakemdrew/DataMiningNotebooks/blob/master/06.%20Classification.ipynb\n",
    "\n",
    "#CappedLinear Regression\n",
    "regr3 = CappedLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
    "                       normalize=False)\n",
    "\n",
    "regest3 = Pipeline(\n",
    "    [('scaler',ML_std_scalr),\n",
    "     ('regr',regr3)]\n",
    ")\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=959)\n",
    "\n",
    "reg_err3 = cross_val_score(regest3, X_regr, y_regr, cv=cv, scoring = 'neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h0: The mean of MAE of the Gradient Boosting Regressor equals the mean of MAE of the LASSO regressor\n",
    "\n",
    "ha: The mean MAE of the models are different\n",
    "\n",
    "Comparing GradientBoostingRegressor and Lasso, the 95% confidence interval [43411,55324] shows that the interval doesn't includes zero, which implies that the two models are statistically different and we reject null hypothesis. There is evidence to suggest the LASSO model is statistically better at prediciting sale price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wanting to check 3 models, so bonferroni correction for alpha = 0.05 / 2 = 0.025therefore t = 2.76\n",
    "# we reject h0 the models are statistically different, as zero is not in the interval, The LASSO model is the better \n",
    "#regressor model. \n",
    "\n",
    "\n",
    "t = 2.76 / np.sqrt(10)\n",
    "\n",
    "e = abs(reg_err1) - abs(reg_err2) \n",
    "\n",
    "stdtot = np.std(e)\n",
    "\n",
    "dbar = np.mean(e)\n",
    "print ('Range of:', dbar-t*stdtot,dbar+t*stdtot )\n",
    "print (abs(np.mean(reg_err1)), abs(np.mean(reg_err2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h0: The mean of MAE of the LASSO Regressor equals the mean of MAE of the Capped regressor\n",
    "\n",
    "ha: The mean MAE of the models are different\n",
    "\n",
    "Comparing Lasso and CappedLinearRegression, the 95% confidence interval [-10027,-946] shows that the interval doesn't includes zero, which implies that the two models are statistically different and we reject null hypothesis. There is sufficient evidence to sau the Lasso model performs better than the capped linear regression model. But one has to consider the practical significance, as there is roughly a ~50k, given the dynamic range of SALEPRICECPY the model is trying to predict, 1e5 to 9.9e6, 50k may or may not be signficant to the consumers of the model. It is something worth mentioning, especially if the capped linear regression model is eaiser to implement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fail to reject h0, the models are not statistically different, \n",
    "#there is not sufficient evidence to suggest the error from the capped linear regression and the lasso regression \n",
    "# is different. \n",
    "t = 2.76 / np.sqrt(10)\n",
    "\n",
    "e =  abs(reg_err2) - abs(reg_err3)\n",
    "\n",
    "stdtot = np.std(e)\n",
    "\n",
    "dbar = np.mean(e)\n",
    "print ('Range of:', dbar-t*stdtot,dbar+t*stdtot )\n",
    "print (abs(np.mean(reg_err2)), abs(np.mean(reg_err3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 6\n",
    ">*Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the classification task, we will review the most important features in the Logistic Regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset Data for StandardScalar to use Logistic Regression\n",
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values\n",
    "\n",
    "#scale data\n",
    "X = ML_std_scalr.fit_transform(X)\n",
    "\n",
    "clf = lr_clf #Top performing logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2', C=10, solver='lbfgs', random_state=959)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary of the attributes and their coefficients \n",
    "coef_dict = {}\n",
    "for coef, feat in zip(clf.coef_[0,:], ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).columns):\n",
    "    coef_dict[feat] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code adopted from: https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "coef_dict_sorted = {k: v for k, v in sorted(coef_dict.items(), key=lambda item: item[1])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(coef_dict_sorted.keys(), coef_dict_sorted.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart is unreadable, so we will sort our dictionary to find the top ten highest attributes (the negative coefficients don't go below -0.1, so we can focus on positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict\n",
    "coef_top_ten = {k:coef_dict_sorted[k] for k in list(coef_dict_sorted)[-10:] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(coef_top_ten.keys(), coef_top_ten.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Top Ten Weighted Features')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.from_dict.html\n",
    "\n",
    "#Table to view the exact values\n",
    "pd.DataFrame.from_dict(coef_top_ten, orient='index', columns=[\"Weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from isolating the ten attributes with the highest weights, they pertain to either neighborhoods or zip codes.  This isn't surprising, as there are distinct neighborhood names that occur in perhaps one borough.  This is the same with zip code.  We saw earlier that there is overlpa, but we assumed it would be a very helpful feature in classifying the correct borough for our applicaiton. \n",
    "\n",
    "Even though we have the top ten heighest-weighted featuers, let's take a look at what they mean relative to the reference class--Class 0, which is borough 1 (Manhattan).\n",
    "\n",
    "##### NEIGHBORHOOD_GREENWICH VILLAGE-WEST:\n",
    ">NEIGHBORHOOD_GREENWICH VILLAGE-WEST has a weight of 0.39. This is a categorical attribute, therefore we can interpret the weight as a change from the reference value of NEIGHBORHOOD_GREENWICH VILLAGE-WEST 0 to NEIGHBORHOOD_GREENWICH VILLAGE-WEST 1 (yes) increases the odds of the the borough being classified as 0 (Manhattan) by 48%, or a multiplicative factor of 1.48.\n",
    "\n",
    "##### NEIGHBORHOOD_UPPER EAST SIDE:\n",
    ">NEIGHBORHOOD_UPPER EAST SIDE has a weight of 0.40. This is a categorical attribute, therefore we can interpret the weight as a change from the reference value of NEIGHBORHOOD_UPPER EAST SIDE 0 to NEIGHBORHOOD_UPPER EAST SIDE 1 (yes) increases the odds of the the borough being classified as 0 (Manhattan) by 49%, or a multiplicative factor of 1.49.\n",
    "\n",
    "    \n",
    "##### NEIGHBORHOOD_HARLEM-UPPER:\n",
    ">NEIGHBORHOOD_HARLEM-UPPERER has a weight of 0.40. This is a categorical attribute, therefore we can interpret the weight as a change from the reference value of NEIGHBORHOOD_HARLEM-UPPER 0 to NEIGHBORHOOD_HARLEM-UPPER 1 (yes) increases the odds of the the borough being classified as 0 (Manhattan) by 49%, or a multiplicative factor of 1.49.\n",
    "    \n",
    "##### NEIGHBORHOOD_UPPER EAST SIDE:\n",
    ">NNEIGHBORHOOD_UPPER EAST SIDE has a weight of 0.43. This is a categorical attribute, therefore we can interpret the weight as a change from the reference value of NNEIGHBORHOOD_UPPER EAST SIDE 0 to NEIGHBORHOOD_UPPER EAST SIDE 1 (yes) increases the odds of the the borough being classified as 0 (Manhattan) by 54%, or a multiplicative factor of 1.54.\n",
    "    \n",
    "    \n",
    "##### NEIGHBORHOOD_INWOOD:\n",
    ">NNEIGHBORHOOD_UPPER EAST SIDE has a weight of 0.43. This is a categorical attribute, therefore we can interpret the weight as a change from the reference value of NNEIGHBORHOOD_UPPER EAST SIDE 0 to NEIGHBORHOOD_UPPER EAST SIDE 1 (yes) increases the odds of the the borough being classified as  0 (Manhattan) by 54%, or a multiplicative factor of 1.54.\n",
    "    \n",
    "##### ZIPCODE_10031:\n",
    ">ZIPCODE_10031 has a weight of 0.45. This is a categorical attribute, therefore we can interpret the weight as a change from the reference value of ZIPCODE_10031 0 to ZIPCODE_10031 1 (yes) increases the odds of the the borough being classified as 0 (Manhattan) by 57%, or a multiplicative factor of 1.57.\n",
    "    \n",
    "##### ZIPCODE_10029:\n",
    ">ZIPCODE_10029 has a weight of 0.48. This is a categorical attribute, therefore we can interpret the weight as a change from the reference value of ZIPCODE_10029 0 to ZIPCODE_10029 1 (yes) increases the odds of the the borough being classified as 0 (Manhattan) by 62%, or a multiplicative factor of 1.62.\n",
    "    \n",
    "##### NEIGHBORHOOD_HARLEM-EAST:\n",
    ">NEIGHBORHOOD_HARLEM-EAST has a weight of 0.54. This is a categorical attribute, therefore we can interpret the weight as a change from the reference value of NEIGHBORHOOD_HARLEM-EAST 0 to NEIGHBORHOOD_HARLEM-EAST 1 (yes) increases the odds of the the borough being classified as 0 (Manhattan) by 72%, or a multiplicative factor of 1.72.\n",
    "    \n",
    "##### NEIGHBORHOOD_CHELSEA:\n",
    ">NEIGHBORHOOD_CHELSEA has a weight of 0.56. This is a categorical attribute, therefore we can interpret the weight as a change from the reference value of NEIGHBORHOOD_CHELSEA 0 to NEIGHBORHOOD_CHELSEA 1 (yes) increases the odds of the the borough being classified as 0 (Manhattan) by 75%, or a multiplicative factor of 1.75.\n",
    "    \n",
    "##### NEIGHBORHOOD_HARLEM-CENTRAL:\n",
    ">NEIGHBORHOOD_HARLEM-CENTRAL has a weight of 0.65. This is a categorical attribute, therefore we can interpret the weight as a change from the reference value of NEIGHBORHOOD_HARLEM-CENTRAL 0 to NEIGHBORHOOD_HARLEM-CENTRAL 1 (yes) increases the odds of the the borough being classified as 0 (Manhattan) by 92%, or a multiplicative factor of 1.92."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py\n",
    "#https://nbviewer.jupyter.org/github/jakemdrew/EducationDataNC/blob/master/2017/Models/2017SegregatedElementarySchoolCampuses.ipynb\n",
    "#Create a regression estimator with best parameters for cross validation\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=959)\n",
    "\n",
    "regrlasso = Lasso(alpha=10, copy_X=True, fit_intercept=True, max_iter=10000, normalize=True,\n",
    "      positive=False, precompute=True, random_state=0, selection='random',\n",
    "      tol=0.0001, warm_start=True)\n",
    " \n",
    "\n",
    "pipe = Pipeline([('scaler',ML_std_scalr)]) \n",
    "\n",
    "pipe.fit(X_regr, y_regr)\n",
    "\n",
    "X_regr_scl_p = pipe.transform(X_regr)\n",
    "\n",
    "rfecv = RFECV(estimator=regrlasso, step=1, cv=cv, scoring='neg_mean_absolute_error')\n",
    "rfecv.fit(X_regr_scl_p, y_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (LASSO NEG MSE)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "rfe_ft_imp_df = pd.DataFrame({'feature_names':ml_df_feats.columns, 'weights':rfecv.grid_scores_})\n",
    "rfe_ft_imp_df.sort_values(by='weights', inplace=True, ascending=False )\n",
    "\n",
    "top50features = rfe_ft_imp_df.head(50)\n",
    "\n",
    "top50features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    ">*How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?*\n",
    "\n",
    "\n",
    "This model will be very useful for buying or selling real estate in NY. It provides price prediction and trends in each Borough.\n",
    "\n",
    "The measurement of the model value is number of users using the model on daily basis. Usage logs will be captured and analyzed constantly to assess model value and appropriate actions are taken to retain the customers.\n",
    "\n",
    "After training and testing the final model we will make it available for the interested parties or customers by deploying it to production environment. One way to do this is to save the trained Scikit-Learn model using joblib, including the full preprocessing and prediction pipeline into production environment.\n",
    "\n",
    "Option1: This strategy is by wrapping the model with a dedicated REST API end point hosted on any Web server which servers HTTP/HTTPS protocols. This pattern will insulate the interface and easier to upgrade the model with new version.\n",
    "\n",
    "Option2: As various cloud platforms are offering AI/ML hosting platforms. Like Google Cloud Platform (GCP) AI Platform. We will save best model using joblib and upload it to Google Cloud Storage (GCS), then on the Google Clod AI Platform create a new model version, pointing to the GCS file. This will provide load balanced and scaled REST web service.\n",
    "\n",
    "Both options take JSON requests as input data and returns JSON responses containing the predictions. The RESTful API can be consumed by any third-party applications which can consume REST APIs. In addition, we will deploy web application which provides UI to interact with the APIs mentioned above. where users can consume as SaaS. Any software which goes to production need to be monitored to check models live performance and availability. Logs need to be captured for monitoring. Note: It is not always possible to determine the models performance without human analysis. Human intervention is also required to maintain the health of the deployed model.\n",
    "\n",
    "We will refresh the data and train the model every one week, which keep up with seasonal price variation trends by Borough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work\n",
    ">*You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please award accordinly to using grid search techniques on the SALEPRICECPY regression models, and for using a Gradient Boosting Regressor for the regression of SALEPRICECPY with the grid search and the grid search for the logistic regression. \n",
    "\n",
    "Also, please see the below SMOTE analysis for balancing multi-classification for BOROUGH. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE ANALYSIS for Multi-Classification for BOROUGH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting observation we observed for the classification task occured using SMOTE sampling from the imblearn package. We observed mean accuracy of 100% in the case of scaled SMOTE data using the helper CV test function and .99% using unscaled SMOTE data. Even more interesting, we found by just scaling the data, the accuracy was similar to using Scaled SMOTE sampling. \n",
    "\n",
    "This prompted us to explore whether there had been bleed over from the CV sets into the test sets. \n",
    "\n",
    "The following analysis will review a file of hashed values for the features to explore what is really occuring in the CV folds and then perform a 3 CV experiment on SMOTE data, SMOTE Scaled Data and Scaled Original Data using the same logistic regression and the same 3 CV data sets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the original housing data for the BOROUGHS multi level classification problem is largely imbalanced, with class 3 having the largest number of occurances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_num_class = housing_ml_df.BOROUGH.values\n",
    "plot_class_dist(y_num_class) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the SMOTE function has upsampled the data to balance all of the classes for the target value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html#smote-variants\n",
    "\n",
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values\n",
    "\n",
    "smte = SMOTE(sampling_strategy='not majority')\n",
    "X_smote, y_smote = smte.fit_sample(X, y)\n",
    "\n",
    "plot_class_dist(y_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us review the Hashed values of our original encoded data set and perform SMOTE upsampling to see what the algorthim is accomplishing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df_enc_hashed_url = 'https://raw.githubusercontent.com/andrewmejia600/MSDS7331/andrew/RAW_DATA/ml_df_enc_cpy_hash.csv'\n",
    "\n",
    "ml_df_enc_hashed = pd.read_csv(ml_df_enc_hashed_url)\n",
    "\n",
    "ml_df_enc_hashed.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df_enc_hashed.hash_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df_enc_hashed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the classes are remain balanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hash = ml_df_enc_hashed['BOROUGH'].values\n",
    "\n",
    "X_hash = ml_df_enc_hashed.drop(columns = ['BOROUGH', 'Unnamed: 0'], axis = 1).values\n",
    "\n",
    "smte = SMOTE(sampling_strategy='not majority')\n",
    "X_smote_hash, y_smote_hash = smte.fit_sample(X_hash, y_hash)\n",
    "\n",
    "plot_class_dist(y_smote_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our Hashed encoded data, lets perform a 10K stratified split, to ensure we still have even balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sskf = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "sskf.get_n_splits(X_smote_hash, y_smote_hash)\n",
    "\n",
    "test_idx = {}\n",
    "train_idx = {}\n",
    "counter = 1\n",
    "for train_index, test_index in sskf.split(X_smote_hash, y_smote_hash):\n",
    "    \n",
    "    k,v = counter, X_smote_hash[train_index][:,-1]\n",
    "    train_idx.update({k : v})\n",
    "    \n",
    "    k,v = counter, X_smote_hash[test_index][:,-1]\n",
    "    test_idx.update({k : v})\n",
    "    \n",
    "    counter+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now collect all of the hashes from the CV splits into dictionaries for both the Test Folds and the Training Folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from \n",
    "#https://stackoverflow.com/questions/19736080/creating-dataframe-from-a-dictionary-where-entries-have-different-lengths\n",
    "test_folds = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in test_idx.items() ]))\n",
    "test_folds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.melt.html#pandas.DataFrame.melt\n",
    "test_folds_melt = test_folds.copy()\n",
    "test_folds_melt['idx'] = test_folds_melt.index\n",
    "test_folds_melt = test_folds_melt.melt(id_vars=['idx'], value_vars=[1,2,3,4,5,6,7,8,9,10], var_name ='Fold')\n",
    "test_folds_melt.rename(columns = {'Idx' : 'OrigIdx', 'value' : 'SMOTEIdx'}, inplace = True) \n",
    "test_folds_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folds_melt = test_folds_melt.dropna(axis = 0)\n",
    "test_folds_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folds_melt['SMOTEHASH'] = test_folds_melt.SMOTEIdx\n",
    "test_folds_melt = test_folds_melt.drop(columns = 'SMOTEIdx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folds_melt.head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are not any duplicate hashes within the Test Folds, meaning, a hash did not repeat internally in its own fold or externally into another fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folds_melt[test_folds_melt.duplicated(['SMOTEHASH'], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets review the train folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in train_idx.items() ]))\n",
    "train_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds_melt = train_folds.copy()\n",
    "train_folds_melt['idx'] = train_folds_melt.index\n",
    "train_folds_melt = train_folds_melt.melt(id_vars=['idx'], value_vars=[1,2,3,4,5,6,7,8,9,10], var_name = 'Fold')\n",
    "train_folds_melt.rename(columns = {'Idx' : 'OrigIdx', 'value' : 'SMOTEIdx'}, inplace = True) \n",
    "train_folds_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds_melt = train_folds_melt.dropna(axis = 0)\n",
    "train_folds_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds_melt['SMOTEHASH'] = train_folds_melt.SMOTEIdx\n",
    "train_folds_melt = train_folds_melt.drop(columns = 'SMOTEIdx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds_melt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see in the train folds, there are duplicated hashes, this is to be expected with CV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds_melt[train_folds_melt.duplicated(['SMOTEHASH'], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the train folds are making 9 train folds and 1 test fold for our CV split. Again this is expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds_melt.groupby(['SMOTEHASH']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn our attention to how many SMOTE hashes are found in both the training folds and the test folds. We see that there are considerable intersections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_smote_idxs = train_folds_melt.merge(test_folds_melt, how = 'inner', on='SMOTEHASH', suffixes = ['Train', 'Test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_smote_idxs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets see how many are duplicated on the hash. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_smote_idxs_d = all_smote_idxs[all_smote_idxs.duplicated(['SMOTEHASH'], keep=False)].reset_index().drop(columns = ['idxTrain', 'idxTest', 'index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_smote_idxs_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see while the Training fold has a duplicate hash, each test fold is being held out and not making into its own fold for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_smote_idxs_d[all_smote_idxs_d.duplicated(['SMOTEHASH', 'FoldTest'], keep=False)].head(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see how this impacts our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try a method as perscribed by the referenced article https://beckernick.github.io/oversampling-modeling/ of how to time the SMOTE technique. \n",
    "\n",
    "The article essentially states one should only apply SMOTE to the train data set, and then test on a hold out set. \n",
    "\n",
    "We call when we optimized the parameters and used a Stratified 10 K fold on a SMOTE data set where we did not control the SMOTE applying to only the train data set, we got an accuarcy score of 1. This seemed highly suspicious. \n",
    "\n",
    "We will use 5 fold Stratified CV for the interest of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, we see on the original data, the overall accuracy from the 5 fold split is not better than .50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True, random_state=959)\n",
    "lr_clf = LogisticRegression(solver = 'lbfgs', multi_class='multinomial', n_jobs=-1, random_state=959)\n",
    "\n",
    "\n",
    "\n",
    "for train_index, test_index in cv.split(X, y): \n",
    "    \n",
    "    lr_clf.fit(X[train_index],y[train_index])\n",
    "\n",
    "    print(lr_clf.score(X[test_index], y[test_index])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pure SMOTE acually decreased the accuracy accross the folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from \n",
    "#https://beckernick.github.io/oversampling-modeling/\n",
    "\n",
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True, random_state=959)\n",
    "lr_clf = LogisticRegression(solver = 'lbfgs', multi_class='multinomial', n_jobs=-1, random_state=959)\n",
    "\n",
    "for train_index, test_index in cv.split(X, y): \n",
    "    X_smote_train, y_smote_train = smte.fit_sample(X[train_index], y[train_index])\n",
    "    lr_clf.fit(X_smote_train,y_smote_train)\n",
    "    print(lr_clf.score(X[test_index], y[test_index])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply SMOTE to scaled training Data only and test against scaled test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values\n",
    "\n",
    "#Create scaler object \n",
    "ML_std_scalr = StandardScaler()\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True, random_state=959)\n",
    "lr_clf = LogisticRegression(solver = 'lbfgs', multi_class='multinomial', n_jobs=-1, random_state=959)\n",
    "\n",
    "\n",
    "\n",
    "for train_index, test_index in cv.split(X, y): \n",
    "    ML_std_scalr.fit(X)\n",
    "    X_smote_scl = ML_std_scalr.transform(X)\n",
    "    X_smote_train, y_smote_train = smte.fit_sample(X_smote_scl[train_index], y[train_index])\n",
    "    lr_clf.fit(X_smote_train,y_smote_train)\n",
    "    print(lr_clf.score(X_smote_scl[test_index], y[test_index])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we merely scale the data and do not apply SMOTE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True, random_state=959)\n",
    "lr_clf = LogisticRegression(solver = 'lbfgs', multi_class='multinomial', n_jobs=-1, random_state=959)\n",
    "\n",
    "\n",
    "\n",
    "for train_index, test_index in cv.split(X, y): \n",
    "    ML_std_scalr.fit(X)\n",
    "    X_scl = ML_std_scalr.transform(X)\n",
    "    lr_clf.fit(X_scl[train_index],y[train_index])\n",
    "\n",
    "    print(lr_clf.score(X_scl[test_index], y[test_index])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the indices are repeated in the test sets when using stratified k fold and SMOTE sampling. While there is not bleed over within the test folds, we do see bleed over from the train folds into the test folds, but the test folds do not appear to be overlapping. We can say the SMOTE technique from imblearn does work as advertised. \n",
    "\n",
    "We see that controlling for when we conduct SMOTE sampling and test on a hold out set, we do not necessarily see a gain in accuracy performance on this data set. \n",
    "\n",
    "Perhaps what is even more interesting is when we tested SMOTE scaled data v.s. the original data after being scaled. We see the results are identical and SMOTE is not even required for this data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDITIONAL REGRESSION MODELS FOR EXCEPTIONAL POINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_regr = ml_df_enc['SALEPRICE'].values\n",
    "X_regr = ml_df_enc.drop(columns = ['SALEPRICE'], axis = 1).values\n",
    "\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html#sklearn.kernel_approximation.Nystroem\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=959)\n",
    "\n",
    "reg = SGDRegressor()\n",
    "\n",
    "feature_map_nystroem = Nystroem(gamma=.2,\n",
    "                                random_state=1,\n",
    "                                n_components=400)\n",
    "\n",
    "X_regr_transformed = feature_map_nystroem.fit_transform(X_regr)\n",
    "\n",
    "#Set up SVR parameters to test (WARNING: Creates 80 models!!!) \n",
    "loss = ['squared_loss', 'epsilon_insensitive']\n",
    "alphas = [0.0001, 0.1]\n",
    "penalty = ['l2']\n",
    "parameters = {'loss': loss, 'alpha' : alphas, 'penalty': penalty }\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=reg\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X_regr_transformed,y_regr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateRegressionEstimator(regEstimator, X_regr_transformed ,y_regr, cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
