{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.offline.init_notebook_mode()\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.svm import SVC\n",
    "from pandas.plotting import boxplot\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error, mean_squared_error\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "print(__doc__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_url = 'https://raw.githubusercontent.com/andrewmejia600/MSDS7331/master/RAW_DATA/nyc-rolling-sales.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(raw_data_url, encoding=\"utf-8\", converters = {'LAND SQUARE FEET': str.strip, 'GROSS SQUARE FEET' : str.strip, 'SALE PRICE': str.strip  } )\n",
    "raw_data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = raw_data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data['DATEOFSALE'] = pd.to_datetime(housing_data['SALE DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = housing_data.drop(columns = ['Unnamed: 0', 'EASE-MENT', 'APARTMENT NUMBER', 'SALE DATE'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.columns = housing_data.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see there is a lot of white space with some of the columns, helper function to remove whitespace and return df \n",
    "def remove_white_space(cols_list, dataframe): \n",
    "    df = dataframe\n",
    "    for col in cols_list:\n",
    "        df[col] = df[col].str.strip()\n",
    "    return df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to  look at unique levels in object categories \n",
    "def unique_categories(columns_list, dataframe_1): \n",
    "    miss_cat_vars = {}\n",
    "    for var in columns_list: \n",
    "        print(var)\n",
    "        k,v = var,dataframe_1[var].unique()\n",
    "        miss_cat_vars.update({k : v})\n",
    "    return miss_cat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for grouper imputation \n",
    "\n",
    "#Adapted from Jezreal's answer \n",
    "#https://stackoverflow.com/questions/51189962/how-to-replace-0-values-with-mean-based-on-groupby\n",
    "\n",
    "def grouper_impute(dataframe_2, grouper_col = None, grouper_impute = None, replace_val = None, transfrmtn = None): \n",
    "\n",
    "    dataframe_2[grouper_impute] = dataframe_2[grouper_impute].replace(replace_val, np.nan)\n",
    "    dataframe_2[grouper_impute] = dataframe_2[grouper_impute].fillna(dataframe_2.groupby(grouper_col)[grouper_impute].transform(transfrmtn))\n",
    "    return dataframe_2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catagorical_vars = list(housing_data.select_dtypes(include='object').columns)\n",
    "print(catagorical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = remove_white_space(catagorical_vars, housing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are not sure what the '-' value for sale price means, we will omit it from further analysis, as these could be family property transfers or transfers other than sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(housing_data[housing_data['SALEPRICE'] == '-']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are not sure what the '-' value for Landsqurefeet and grosssquarefeet means, we will omit it from further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = housing_data[~((housing_data['LANDSQUAREFEET'] == '-') | (housing_data['GROSSSQUAREFEET'] == '-') | (housing_data['SALEPRICE'] == '-'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = housing_data[(housing_data['ZIPCODE'] != 0) & (housing_data['TOTALUNITS'] != 0) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = housing_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_exclude = housing_data[((housing_data['LANDSQUAREFEET'] == '-') | (housing_data['GROSSSQUAREFEET'] == '-') | (housing_data['SALEPRICE'] == '-'))]\n",
    "housing_data_exclude.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html\n",
    "housing_data = housing_data.astype({'SALEPRICE': 'int64', 'LANDSQUAREFEET' : 'int64', 'GROSSSQUAREFEET' : 'int64', 'ZIPCODE' : 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_of_day = housing_data.DATEOFSALE.astype(str).str.slice(start=-2).astype(int)\n",
    "\n",
    "housing_data['TIMEOFMONTH'] = pd.cut(string_of_day, [1,14,15,31], labels=[1,2,3], include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.TIMEOFMONTH = housing_data.TIMEOFMONTH.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www1.nyc.gov/assets/finance/jump/hlpbldgcode.html#A\n",
    "BUILD_CLASS_DICT_ = {'A' : 'DWELLING', 'B' : 'DWELLING', 'C' : 'MULTI_DWELLING_NO_ELV', 'D' : 'MULTI_DWELLING_ELV', 'E' : 'WAREHOUSE', 'F' : 'FACTORY', 'G' : 'GARAGE_P_LOT', 'H' : 'HOTEL', 'I' : 'HOSIPTAL', 'J' : 'THEATRE', 'K' : 'REATAIL', 'L' : 'LOFT', 'M' : 'RELIGIOUS', 'N' : 'SOCIAL_INSTITUTION', 'O' : 'OFFICE', 'P' : 'COMMUNITY_CENTER', 'Q' : 'PUBLIC_REC', 'R' : 'COMMERCIAL' , 'S' : 'DWELLING_RETAIL', 'T' : 'PORT', 'U' : 'UTILITY', 'V' : 'ZONED', 'W' : 'SCHOOL', 'Y' : 'PUBLIC_SAFETY', 'Z' : 'OTHER'}\n",
    "\n",
    "housing_data['BUILDCLASSGENER'] = housing_data.BUILDINGCLASSATPRESENT.astype(str).str.slice(start=0, stop=1).map(BUILD_CLASS_DICT_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = grouper_impute(dataframe_2 = housing_data, grouper_col = 'ZIPCODE', grouper_impute = 'SALEPRICE', replace_val = 0, transfrmtn = 'median')\n",
    "\n",
    "housing_data = grouper_impute(dataframe_2 = housing_data, grouper_col = 'ZIPCODE', grouper_impute = 'YEARBUILT', replace_val = 0, transfrmtn = 'median')\n",
    "\n",
    "\n",
    "housing_data = grouper_impute(dataframe_2 = housing_data, grouper_col = 'ZIPCODE', grouper_impute = 'LANDSQUAREFEET', replace_val = 0, transfrmtn = 'median')\n",
    "\n",
    "housing_data = grouper_impute(dataframe_2 = housing_data, grouper_col = 'ZIPCODE', grouper_impute = 'GROSSSQUAREFEET', replace_val = 0, transfrmtn = 'median')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.pad.html\n",
    "housing_data['DECADEBUILT'] = housing_data.YEARBUILT.astype(str).str.slice(start=0,stop=3).str.pad(width = 4, side='right', fillchar = str(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = housing_data.astype({'SALEPRICE': 'int64', 'LANDSQUAREFEET' : 'int64', \n",
    "                                    'GROSSSQUAREFEET' : 'int64', 'ZIPCODE' : 'object',\n",
    "                                   'YEARBUILT' : 'int64', 'RESIDENTIALUNITS' : 'int64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accounting for duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_records = housing_data[housing_data.duplicated(keep = False)]\n",
    "duplicate_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Duplicate Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = housing_data[~housing_data.duplicated(keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Imuptations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.groupby(\"BOROUGH\").median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.groupby(\"ZIPCODE\").median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.groupby(\"BOROUGH\",as_index=False).agg({'ZIPCODE': lambda x: x.mode()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.groupby(\"BOROUGH\",as_index=False).agg({'RESIDENTIALUNITS': lambda x: x.mode()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.groupby('DATEOFSALE').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feat = list(housing_data.select_dtypes(include='object').columns)\n",
    "cat_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories(columns_list = cat_feat, dataframe_1 = housing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(housing_data['RESIDENTIALUNITS'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(housing_data['TOTALUNITS'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(housing_data.BUILDINGCLASSCATEGORY.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instances where building class at present changed from time of sale -- reclassification\n",
    "housing_data[housing_data.BUILDINGCLASSATPRESENT != housing_data.BUILDINGCLASSATTIMEOFSALE ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://www.featureranking.com/tutorials/machine-learning-tutorials/information-gain-computation/\n",
    "#https://nbviewer.jupyter.org/github/jakemdrew/MachineLearningExtras/blob/master/LFW%20Dataset%20and%20Class%20Imbalance.ipynb\n",
    "\n",
    "def gini_index(y):\n",
    "    probs = pd.value_counts(y,normalize=True)\n",
    "    return 1 - np.sum(np.square(probs))\n",
    "\n",
    "def plot_class_dist(y, target_label = None):\n",
    "    fig, axarr = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    class_ct = len(np.unique(y))\n",
    "    vc = pd.value_counts(y)\n",
    "    print('Total Records', len(y))\n",
    "    print('Total Classes:', class_ct)\n",
    "    print('Class Gini Index', gini_index(y))\n",
    "    print('Smallest Class Id:',vc.idxmin(),'Records:',vc.min())\n",
    "    print('Largest Class Id:',vc.idxmax(),'Records:',vc.max())\n",
    "    print('Accuracy when Guessing:', np.round( (1 / len(np.unique(y))) * 100, 2), '%')\n",
    "\n",
    "    sns.distplot(y, ax=axarr[0], bins=class_ct).set_title('Target Class Distribution:', target_label);\n",
    "    sns.distplot(y, ax=axarr[1], kde=False, bins=class_ct).set_title('Target Class Counts:', target_label);\n",
    "    \n",
    "    \n",
    "# This function creates dummy encodings from a lsit of features of interest and returns a dataframe     \n",
    "def create_dummy_encod(ml_df,features_of_interest, drop_first_cat=True, sparsity=True): \n",
    "    tmp_cont = []\n",
    "    ml_data_copy = ml_df.copy()\n",
    "    for feat in features_of_interest: \n",
    "        tmp_df = pd.get_dummies(ml_data_copy[feat],prefix=str(feat),sparse=sparsity,drop_first=drop_first_cat)\n",
    "        tmp_cont.append(tmp_df)\n",
    "        feat_df = pd.concat(tmp_cont,axis=1)\n",
    "        ml_df = pd.concat([ml_data_copy,feat_df], axis=1)\n",
    "        ml_df = ml_df.drop(columns = features_of_interest, axis = 1)\n",
    "    return ml_df \n",
    "\n",
    "\n",
    "\n",
    "#Adopted from \n",
    "#https://nbviewer.jupyter.org/github/jakemdrew/MachineLearningExtras/blob/master/LFW%20Dataset%20and%20Class%20Imbalance.ipynb\n",
    "\n",
    "# This function provides a way to use stratiefied cv with the test models function below using the default score of accuarcy \n",
    "\n",
    "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=959)\n",
    "\n",
    "def stratified_cross_validate(model, X, y, cv=cv):\n",
    "    start = time.time()\n",
    "    cv_results = cross_validate(model, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "    elapsed_time = (time.time() - start) \n",
    "    print ('Fold Scores:')\n",
    "    print(' ')\n",
    "    print(cv_results['test_score'])\n",
    "    print(' ')\n",
    "    print('Mean Accuracy: ', cv_results['test_score'].mean())\n",
    "    print('Mean Fit Time: ', cv_results['fit_time'].mean())\n",
    "    print('Mean Score Time: ', cv_results['score_time'].mean())\n",
    "    print('CV Time: ', elapsed_time)\n",
    "    return\n",
    "\n",
    "\n",
    "#This function provides a quick method to to performe strativied cross validation model comparision \n",
    "\n",
    "def test_models(X, y, models, model_names):\n",
    "    for model, model_name in zip(models,model_names):\n",
    "        print(model_name)\n",
    "        print('--------------------------------')\n",
    "        stratified_cross_validate(model,X,y)\n",
    "        print(' ')\n",
    "\n",
    "#This function will plot PCs based on the length of features in the dataframe or change to how many features you wish to input \n",
    "def plot_pca(X,var_ratio_pcs = 60):\n",
    "    # Perform PCA on the data to reduce the number of initial features \n",
    "    # and to remove correlations that are common between pixel features \n",
    "    pca = PCA(n_components=X.shape[1])\n",
    "    pca.fit(X)\n",
    "\n",
    "    # Inspect the explained variances to determine how many components to use  \n",
    "    plt.subplots(figsize=(8, 8))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance');\n",
    "    print('Cumulative Explained variance explained with :', var_ratio_pcs , 'components:',sum(pca.explained_variance_ratio_[0:var_ratio_pcs]) )\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many classes we have and their frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_num_class = housing_data.BOROUGH.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see we have a highly imbalanced data set for Borough values for the outcomes and there is a 20% chance of guessing the class correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_dist(y_num_class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_of_int = [ 'NEIGHBORHOOD', 'BUILDINGCLASSCATEGORY', 'ZIPCODE', 'DECADEBUILT','TAXCLASSATPRESENT', 'BUILDINGCLASSATPRESENT', 'TIMEOFMONTH','BUILDCLASSGENER']\n",
    "housing_ml_df = housing_data[['NEIGHBORHOOD','BUILDINGCLASSCATEGORY','ZIPCODE', 'DECADEBUILT', 'TAXCLASSATPRESENT', \n",
    "                              'BUILDINGCLASSATPRESENT', 'TIMEOFMONTH','BUILDCLASSGENER', \n",
    "                              'RESIDENTIALUNITS', 'COMMERCIALUNITS', 'TOTALUNITS',\n",
    "                              'LANDSQUAREFEET', 'GROSSSQUAREFEET','YEARBUILT',\n",
    "                              'SALEPRICE','BOROUGH']]\n",
    "ml_df_enc = create_dummy_encod(ml_df = housing_ml_df,features_of_interest = feat_of_int, drop_first_cat=True, sparsity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df_enc.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create scaler object \n",
    "ML_std_scalr = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ml_df_enc['BOROUGH'].values\n",
    "X = ml_df_enc.drop(columns = ['BOROUGH'], axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mods = [\n",
    "    #XGBClassifier(),\n",
    "    GaussianNB(), \n",
    "    MultinomialNB(),\n",
    "    SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3,max_iter=5, \n",
    "                  tol=None, n_jobs=-1, random_state=42),\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, \n",
    "                           n_jobs=-1, random_state=42),\n",
    "    LinearSVC(random_state=42),\n",
    "    LogisticRegression(n_jobs=-1, random_state=42)\n",
    "]\n",
    "\n",
    "mod_names = ['GaussianNB','MultinomialNB','SGDClassifier',\n",
    "               'RandomForestClassifier','LinearSVC','LogisticRegression']\n",
    "\n",
    "test_models(X,y, models = mods, model_names = mod_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect confusion matrix will only show shaded squares across the matrix diagonal.\n",
    "\n",
    "However, the confusion matrix below shows that many class predictions are getting confused with the larger classes 2,3,4.\n",
    "\n",
    "This is evident by the high number of off-diagonal shaded boxes for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from \n",
    "#https://nbviewer.jupyter.org/github/jakemdrew/MachineLearningExtras/blob/master/LFW%20Dataset%20and%20Class%20Imbalance.ipynb\n",
    "\n",
    "# Break our data into 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=959, stratify=y)\n",
    "\n",
    "# Use logstic regression to build a model and predict on the test set\n",
    "lr_clf = LogisticRegression(solver='liblinear', random_state=959)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "\n",
    "# Create a confusion matrix to see what classes the logistic regression is getting wrong \n",
    "\n",
    "\n",
    "\n",
    "plt.subplots(figsize=(10, 10))\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.imshow(cm_normalized,cmap=plt.get_cmap('Blues'),aspect='auto')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the features makes a big improvement to accuaracy \n",
    "X = ML_std_scalr.fit_transform(X)\n",
    "\n",
    "plot_pca(X,var_ratio_pcs = 400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from \n",
    "#https://nbviewer.jupyter.org/github/jakemdrew/MachineLearningExtras/blob/master/LFW%20Dataset%20and%20Class%20Imbalance.ipynb\n",
    "\n",
    "# Break our data into 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=959, stratify=y)\n",
    "\n",
    "# Use logstic regression to build a model and predict on the test set\n",
    "lr_clf = LogisticRegression(solver='liblinear', random_state=959)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "\n",
    "# Create a confusion matrix to see what classes the logistic regression is getting wrong \n",
    "\n",
    "\n",
    "\n",
    "plt.subplots(figsize=(10, 10))\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.imshow(cm_normalized,cmap=plt.get_cmap('Blues'),aspect='auto')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html#smote-variants\n",
    "\n",
    "smte = SMOTE(sampling_strategy='not majority')\n",
    "X_smote, y_smote = smte.fit_sample(X, y)\n",
    "\n",
    "plot_class_dist(y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = RandomUnderSampler()\n",
    "X_us, y_us = us.fit_sample(X, y)\n",
    "\n",
    "# Plot the new class distributions for y using the same funnction as above. \n",
    "plot_class_dist(y_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_clf = LogisticRegression(n_jobs=-1, random_state=959)\n",
    "\n",
    "print('Original Dataset Scaled')\n",
    "print('----------------------------------------------------------------')\n",
    "stratified_cross_validate(lr_clf, X, y)\n",
    "print('\\nOversample all Classes to Majority Dataset')\n",
    "print('----------------------------------------------------------------')\n",
    "stratified_cross_validate(lr_clf, X_smote, y_smote)\n",
    "print('\\nMinority Undersampled Dataset')\n",
    "print('----------------------------------------------------------------')\n",
    "stratified_cross_validate(lr_clf, X_us, y_us)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=959)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adopted from https://github.com/jakemdrew/DataMiningNotebooks/blob/master/07.%20Regression.ipynb\n",
    "\n",
    "#Function for Root mean squared error\n",
    "#https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "def rmse(y_actual, y_predicted):\n",
    "    return np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "#Function for Mean Absolute Percentage Error (MAPE) - Untested\n",
    "#Adapted from - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "def mape(y_actual, y_predicted): \n",
    "    mask = y_actual != 0\n",
    "    return (np.fabs(y_actual - y_predicted)/y_actual)[mask].mean() * 100\n",
    "\n",
    "#Create scorers for rmse and mape functions\n",
    "mae_scorer = make_scorer(score_func=mean_absolute_error, greater_is_better=False)\n",
    "rmse_scorer = make_scorer(score_func=rmse, greater_is_better=False)\n",
    "mape_scorer = make_scorer(score_func=mape, greater_is_better=False)\n",
    "mse_scorer = make_scorer(score_func = mean_squared_error, greater_is_better=False)\n",
    "\n",
    "#Make scorer array to pass into cross_validate() function for producing mutiple scores for each cv fold.\n",
    "errorScoring = {'MAE':  mae_scorer, \n",
    "                'RMSE': rmse_scorer,\n",
    "                'MAPE': mape_scorer, \n",
    "                'MSE' : mse_scorer\n",
    "               } \n",
    "\n",
    "def EvaluateRegressionEstimator(regEstimator, X, y, cv):\n",
    "    \n",
    "    scores = cross_validate(regEstimator, X, y, scoring=errorScoring, cv=cv, return_train_score=True)\n",
    "\n",
    "    #cross val score sign-flips the outputs of MAE\n",
    "    # https://github.com/scikit-learn/scikit-learn/issues/2439\n",
    "    scores['test_MAE'] = scores['test_MAE'] * -1\n",
    "    scores['test_MAPE'] = scores['test_MAPE'] * -1\n",
    "    scores['test_RMSE'] = scores['test_RMSE'] * -1\n",
    "    scores['test_MSE'] = scores['test_MSE'] * -1\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    maeAvg = scores['test_MAE'].mean()\n",
    "    print_str = \"The average MAE for all cv folds is: \\t\\t\\t {maeAvg:.5}\"\n",
    "    print(print_str.format(maeAvg=maeAvg))\n",
    "\n",
    "    #print mean test_MAPE for all folds\n",
    "    scores['test_MAPE'] = scores['test_MAPE']\n",
    "    mape_avg = scores['test_MAPE'].mean()\n",
    "    print_str = \"The average MAE percentage (MAPE) for all cv folds is: \\t {mape_avg:.5}\"\n",
    "    print(print_str.format(mape_avg=mape_avg))\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    RMSEavg = scores['test_RMSE'].mean()\n",
    "    print_str = \"The average RMSE for all cv folds is: \\t\\t\\t {RMSEavg:.5}\"\n",
    "    print(print_str.format(RMSEavg=RMSEavg))\n",
    "    print('*********************************************************')\n",
    "    \n",
    "    #print mean MAE for all folds \n",
    "    MSEavg = scores['test_MSE'].mean()\n",
    "    print_str = \"The average MSE for all cv folds is: \\t\\t\\t {MSEavg:.5}\"\n",
    "    print(print_str.format(MSEavg=MSEavg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['MAE'] = scores['test_MAE']\n",
    "    scoresResults['MAPE'] = scores['test_MAPE']\n",
    "    scoresResults['RMSE'] = scores['test_RMSE']\n",
    "    scoresResults['MSE'] = scores['test_MSE']\n",
    "    return scoresResults\n",
    "\n",
    "class CappedLinearRegression(LinearRegression):\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.clip(super(CappedLinearRegression, self).predict(X), 1, 2.21e9) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_regr = ml_df_enc['SALEPRICE'].values\n",
    "X_regr = ml_df_enc.drop(columns = ['SALEPRICE'], axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Linear Regression object and perform a grid search to find the best parameters\n",
    "linreg = CappedLinearRegression()\n",
    "parameters = {'normalize':(True,False), 'fit_intercept':(True,False)}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=linreg\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X_regr,y_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create CappedLinearRegression predictions between 0 and 100% using the best parameters for our Linear Regression object\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateRegressionEstimator(regEstimator, X_regr,y_regr, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running 10 CV with SVR will take more than 308s on this machine, I wil use 10 CV SDG using the RBF kernel from Nystroem for grid searching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=959)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html#sklearn.kernel_approximation.Nystroem\n",
    "\n",
    "reg = SGDRegressor()\n",
    "\n",
    "feature_map_nystroem = Nystroem(gamma=.2,\n",
    "                                random_state=1,\n",
    "                                n_components=400)\n",
    "\n",
    "X_regr_transformed = feature_map_nystroem.fit_transform(X_regr)\n",
    "\n",
    "#Set up SVR parameters to test (WARNING: Creates 80 models!!!) \n",
    "loss = ['squared_loss', 'epsilon_insensitive']\n",
    "alphas = [0.0001, 0.1]\n",
    "penalty = ['l2']\n",
    "parameters = {'loss': loss, 'alpha' : alphas, 'penalty': penalty }\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=reg\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X_regr_transformed,y_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateRegressionEstimator(regEstimator, X_regr_transformed ,y_regr, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Lasso(fit_intercept=True, normalize=True,copy_X=True\n",
    "          , max_iter=10000, precompute=True, tol=0.0001, random_state=0)\n",
    "\n",
    "#Test parameters \n",
    "alpha = [0.001, 0.1, 1, 10, 20]\n",
    "selection = ['cyclic','random']\n",
    "warm_start = [True, False]\n",
    "parameters = {'alpha': alpha, 'selection': selection, 'warm_start': warm_start}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=reg\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X_regr,y_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a regression estimator with best parameters for cross validation\n",
    "regEstimator = Lasso(alpha=20, copy_X=True, fit_intercept=True, max_iter=10000, normalize=True,\n",
    "      positive=False, precompute=True, random_state=0, selection='random',\n",
    "      tol=0.0001, warm_start=True)\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "EvaluateRegressionEstimator(regEstimator, X_regr,y_regr, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the maximum price pridcted?\n",
    "regEstimator = Lasso(alpha=20, copy_X=True, fit_intercept=True, max_iter=10000, normalize=True,\n",
    "      positive=False, precompute=True, random_state=0, selection='random',\n",
    "      tol=0.0001, warm_start=True)\n",
    "\n",
    "regEstimator.fit(X_regr,y_regr)\n",
    "yhat = regEstimator.predict(X_regr)\n",
    "print('Target Max: ', y_regr.max())\n",
    "print(\"Yhat Max: \", yhat.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge(fit_intercept=True, normalize=True,copy_X=True\n",
    "          , max_iter=1000, tol=0.0001, random_state=0)\n",
    "\n",
    "#Test parameters \n",
    "alpha = [0.001, 0.1, 1, 5, 10, 20]\n",
    "solver = [ 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "parameters = {'alpha': alpha, 'solver': solver}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=reg\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X_regr,y_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a regression estimator with best parameters for cross validation\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "EvaluateRegressionEstimator(regEstimator, X_regr,y_regr, cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
